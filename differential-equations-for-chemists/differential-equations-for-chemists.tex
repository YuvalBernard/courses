\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,graphicx,enumerate,xcolor}
\usepackage[tikz]{mdframed}

%%%%%%%%%% Start TeXmacs macros
\catcode`\>=\active \def>{
\fontencoding{T1}\selectfont\symbol{62}\fontencoding{\encodingdefault}}
\newcommand{\comma}{{,}}
\newcommand{\mathd}{\mathrm{d}}
\newcommand{\mathe}{\mathrm{e}}
\newcommand{\mathi}{\mathrm{i}}
\newcommand{\mathpi}{\pi}
\newcommand{\nequiv}{\mathrel{\not\equiv}}
\newcommand{\nobracket}{}
\newcommand{\nocomma}{}
\newcommand{\nospace}{}
\newcommand{\nosymbol}{}
\newcommand{\tmcolor}[2]{{\color{#1}{#2}}}
\newcommand{\tmem}[1]{{\em #1\/}}
\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
\newcommand{\tmscript}[1]{\text{\scriptsize{$#1$}}}
\newcommand{\tmsession}[3]{{\tt#3}}
\newcommand{\tmstrong}[1]{\textbf{#1}}
\newcommand{\tmtextbf}[1]{\text{{\bfseries{#1}}}}
\newcommand{\tmtextit}[1]{\text{{\itshape{#1}}}}
\newcommand{\tmxspace}{\hspace{1em}}
\newenvironment{enumeratenumeric}{\begin{enumerate}[1.] }{\end{enumerate}}
\newenvironment{enumerateromancap}{\begin{enumerate}[I.] }{\end{enumerate}}
\newenvironment{itemizedot}{\begin{itemize} \renewcommand{\labelitemi}{$\bullet$}\renewcommand{\labelitemii}{$\bullet$}\renewcommand{\labelitemiii}{$\bullet$}\renewcommand{\labelitemiv}{$\bullet$}}{\end{itemize}}
\newenvironment{itemizeminus}{\begin{itemize} \renewcommand{\labelitemi}{$-$}\renewcommand{\labelitemii}{$-$}\renewcommand{\labelitemiii}{$-$}\renewcommand{\labelitemiv}{$-$}}{\end{itemize}}
\mdfsetup{linecolor=black,linewidth=0.5pt,skipabove=0.5em,skipbelow=0.5em,hidealllines=true,innerleftmargin=0pt,innerrightmargin=0pt,innertopmargin=0pt,innerbottommargin=0pt}
\newcommand{\nonconverted}[1]{\mbox{}}
\newmdenv[hidealllines=false,innertopmargin=1ex,innerbottommargin=1ex,innerleftmargin=1ex,innerrightmargin=1ex]{tmframed}
\newmdenv[hidealllines=false,innertopmargin=1ex,innerbottommargin=1ex,innerleftmargin=1ex,innerrightmargin=1ex]{tmornamented}
%%%%%%%%%% End TeXmacs macros

%

\newcommand{\sectionpostsep}{{\sectionalpostsep}}

\begin{document}

\title{Differential Equations for Chemists}

\maketitle

{\tableofcontents}

\part*{Preface: Complex numbers and functions}

\section*{Recollections and definitions}\marginpar{lec 1 6.11.22}

Complex numbers are denoted by $\mathbb{C}= \{ x + \mathi y|x, y \in
\mathbb{R} \}$ in Cartesian representation. In Polar representation, we have:

\begin{center}
  \raisebox{-0.155236337926158\height}{\includegraphics[width=4.93945953036862cm,height=2.85266627312082cm]{differential-equations-for-chemists-1.pdf}}
\end{center}

The complex number $z$ can be written using both $(x, y)$ and $(r, \theta)$,
so that
\begin{eqnarray*}
  x & = & r \cos \theta\\
  y & = & r \sin \theta\\
  r & = & \sqrt{x^2 + y^2}\\
  \theta & = & \arctan \frac{y}{x}, \qquad \tmop{if} \quad x > 0
\end{eqnarray*}
Recall that the {\tmem{complex conjugate}} $\bar{z}$ is defined by: $\bar{z} =
x - \mathi y$ and that the {\tmem{modulus}} squared is
\[ z \cdot \bar{z} = \| z^{\nosymbol} \|^2 = r^2 = x^2 + y^2 \]
so
\[ \| z \| = \sqrt{x^2 + y^2} \quad . \]
Another notation:
\[ \frac{1}{z} = \frac{\bar{z}}{\| z \|^2} \]
\begin{eqnarray*}
  z^2 = (r \cos \theta + \mathi r \sin \theta)^2 & = & r^2 (\cos^2 \theta -
  \sin^2 \theta + 2 \mathi \sin \theta \cos \theta)\\
  & = & r^2 (\cos 2 \theta + \mathi \sin 2 \theta)
\end{eqnarray*}
{\underline{De Moivre's formula:}}
\begin{enumeratenumeric}
  \item $z^n = r^n (\cos n \theta + \mathi \sin n \theta), \quad n \in
  \mathbb{N}$ (integer)
  
  \item  if $z^n = r (\cos \theta + \mathi \sin \theta)$ then:
  \[ z = r^{\frac{1}{n}} \left( \cos \frac{\theta + 2 k \mathpi}{n} + \mathi
     \sin \frac{\theta + 2 k \mathpi}{n} \right), \quad 0 \leq k \leq n - 1 \]
  yields $n$ distinct roots.
\end{enumeratenumeric}
{\underline{Examples:}}
\begin{enumeratenumeric}
  \item $z^3 = 1$. [$r = 1$ and $\theta = 0$]
  
  \begin{center}
    \raisebox{-0.500035993492377\height}{\includegraphics[width=2.34732060868425cm,height=2.2778761642398cm]{differential-equations-for-chemists-2.pdf}}
  \end{center}
  
  then $z = \cos \frac{2 k \mathpi}{3} + \mathi \sin \frac{2 k \mathpi}{3}$
  where $k = 0, \tmcolor{blue}{1}, \tmcolor{red}{2}$
  
  \item $z^n = 1$. $z = \cos \frac{2 \mathpi k}{n} + \mathi \sin \frac{2
  \mathpi k}{n}$ where $0 \leq k \leq n - 1$
  
  \begin{center}
    \raisebox{-0.5\height}{\includegraphics[width=2.39974419519874cm,height=2.38295290568018cm]{differential-equations-for-chemists-3.pdf}}
  \end{center}
  
  $n$ points are distributed equally on the circle.
  
  \item $z^3 = - 8 - 8 \mathi$. $r = \sqrt{64 + 64} = \sqrt{128} = 8 \sqrt{2}$
  and $\theta = 5 \mathpi / 4$.
  
  \begin{center}
    \raisebox{-0.654447765302712\height}{\includegraphics[width=3.35283025055752cm,height=3.33576019939656cm]{differential-equations-for-chemists-4.pdf}}
  \end{center}
  
  so
  \[ z = \left( 8 \sqrt{2} \right)^{1 / 3} \cdot \left( \cos \frac{\frac{5
     \mathpi}{4} + 2 k \mathpi}{3} + \sin \frac{\frac{5 \mathpi}{4} + 2 k
     \mathpi}{3} \right) \]
  \[ z = 2 \cdot 2^{1 / 6}  \left( \cos \frac{5 \mathpi + 8 k \mathpi}{12} +
     \sin \frac{5 \mathpi + 8 k \mathpi}{12} \right) \]
  \begin{center}
    \raisebox{-0.500078901081137\height}{\includegraphics[width=5.69642857142857cm,height=4.26044536271809cm]{differential-equations-for-chemists-5.pdf}}
  \end{center}
  
  All points are shifted by $5 \mathpi / 12$.
\end{enumeratenumeric}

\section*{Complex analysis}

If we have two points, $z_1$ and $z_2$, the distance between them is $z_1 -
z_2 = z_1 + (- z_2)$.



\begin{center}
  \raisebox{-0.339473458792508\height}{\includegraphics[width=8.08623573396301cm,height=4.40290895972714cm]{differential-equations-for-chemists-6.pdf}}
\end{center}

\

In other words, $\| z_1 - z_2 \|$ is the distance between $z_1$ and $z_2$ in
the complex plane, which is a non-negative real number.

Can we use the distance function to define limits and continuity functions
etc{\ldots}?

\subsection*{Limit of a complex sequence}

If given the sequence of numbers $\{ z_n \} |^{\infty}_{n = 1} \subseteq
\mathbb{C}$, we say $z_n \rightarrow w$, $w \in \mathbb{C}$, if $\| z_n - w \|
\rightarrow 0$ as $n \rightarrow \infty$.

For example, given $z = \frac{1 + 3 \mathi}{n}$, as $n \rightarrow \infty$ the
modulus $\| z_n \| = \frac{\sqrt{10}}{n} \rightarrow 0$ \ so the sequence goes
to zero in the complex plane.

\subsection*{Limit of a series of complex numbers}

Given the sequence $\{ z_n \}_{n = 1}^{\infty}$, the series $\sum_{n =
1}^{\infty} z_n$ converges to $u$ if:
\[ \left\| \sum_{n = 1}^k z_n - u \right\| \xrightarrow[k \rightarrow
   \infty]{} 0 \]
For example, given $z_n = z^n$ where $z \in \mathbb{C}$. If $\| z \| < 1$, we
have:
\begin{eqnarray*}
  \sum_{n = 0}^k z^n & = & 1 + z + z^2 + \cdots + z^k = \frac{1 - z^{k + 1}}{1
  - z}
\end{eqnarray*}
\[ (1 + z + \cdots + z^k) (1 - z) = 1 - z^{k + 1} \]
Note that if $\| z \| < 1$ then $\| z^{k + 1} \| \rightarrow_{\nosymbol} 0$ as
$k \rightarrow \infty$. So
\[ \left\| \sum_{n = 1}^k z^n - \frac{1}{1 - z} \right\| = \left\| \frac{-
   z^{k + 1}}{1 - z} \right\| \xrightarrow[k \rightarrow \infty]{} 0 \]


Another example: $z = \frac{1}{2} + \frac{1}{2} \mathi$. The norm is $\| z \|
= \frac{1}{4} + \frac{1}{4} = \frac{1}{2} < 1$. So
\[ \sum_{k = 1}^{\infty} \left( \frac{1 + \mathi}{2} \right)^k = \frac{1}{1 -
   \left( \frac{1}{2} + \frac{1}{2} \mathi \right)} = \frac{1}{\frac{1}{2} -
   \frac{1}{2} \mathi} = \frac{\frac{1}{2} + \frac{1}{2} \mathi}{\frac{1}{2}}
   = 1 + \mathi \]

\subsection*{Complex (and analytic) functions}

If we have a function $f$ such that $f : \mathbb{C} \rightarrow \mathbb{C}$,
we say that $w$ is the limit of $f$ at $z_0$ if $\| f (z) - w \| \rightarrow
0$ as $z \rightarrow z_0$.

Define {\tmem{continuity}}: $f$ is continuous at the point $z_0$ if the limit
$\lim_{z \rightarrow z_0} f (z)$ exists and equals $f (z_0)$.

\tmtextbf{Definition:} $f : \mathbb{C} \rightarrow \mathbb{C}$ is
{\tmem{analytic}} at $z_0$ if for the limit:
\[ \lim_{h \rightarrow 0}  \frac{f (z_0 + h) - f (z_0)}{h} \]
exists and $h \in \mathbb{C}$. Then we denote the limit by $f' (z_0) =
\frac{\mathd f}{\mathd z} |_{z = z_0}$.

If $f$ is analytic at every point we say $f$ is an analytic function.

Turns out that if $f' (z_0)$ exists then so does $f^{(n)} (z) \tmxspace
\forall n$ [all higher derivatives of $f$ also exist].

\subsection*{Reminder: Taylor series}

A (real) {\tmem{power series}} is a series of the form $\sum_{n = 0} a_n x^n$
which converges for some $| x | < r$ ($x \in \mathbb{R}$).

{\underline{Taylor's theorem:}}

\begin{tmframed}
  If $f : \mathbb{R} \rightarrow \mathbb{R}$ is infinitely differentiable in a
  neighborhood (nbhd) of some point $a$, then we can represent $f$
  {\tmem{uniquely}} as a power series of the form:
  \[ f (x) = \sum_{n = 0}^{\infty} C_n (x - a)^n \]
  where
  \[ C_n = \frac{f^{(n)} (a)}{n!} \]
\end{tmframed}

In particular, if $a = 0$ we get the special case
\[ f (x) = \frac{\sum f^{(n)} (0)}{n!} x^n \]
(also called the Maclaurin series).

{\underline{Example:}}

$f (x) = \sum_{n = 0}^{\infty} x^n$ where $x \in \mathbb{R}$ and $| x | < 1$.
We know that
\[ f (x) = \sum_{n = 0}^{\infty} x^n = \frac{1}{1 - x} \]
So $f (x) = \sum_{n = 0}^{\infty} x^n$ must be a Taylor series.

\subsection*{Taylor theorem for analytic functions}

If $f$ is analytic (everywhere) then it can be represented as a power series
$\sum C_n z^n$ where $C_n = \frac{f^{(n)} (0)}{n!}$.

We can use convergent series to define complex analogues of real functions
which{} have Taylor series.

{\underline{Examples:}}
\begin{enumeratenumeric}
  \item $f (x) = \mathe^x$. Because
  \[ \left( \frac{\mathd^n}{\mathd x^n} \mathe^x \right)_{x = 0} = \nospace 1
     \quad \forall n \]
  we get
  \[ C_n = \frac{1}{n!} \]
  Which means its Taylor series is $\sum_{n = 0}^{\infty} \frac{x^n}{n!} =
  \mathe^x$.
  
  We can use this to define a complex function:
  \[ \mathe^z = \sum_{n = 0}^{\infty} \frac{z^n}{n!} \]
  which in fact does converge for all $z \in \mathbb{C}$.
  
  In fact $\mathe^z$ ``behaves'' like an exponential function as we have:
  \begin{eqnarray}
    \mathe^{z_1 + z_2} & = & \mathe^{z_1} \cdot \mathe^{z_2}
    \label{exponential-identities} \\
    \mathe^{z_1 \cdot z_2} & = & (\mathe^{z_1})^{z_2} \nonumber
  \end{eqnarray}
  \item \marginpar{lec 2 7.11.22}$f (x) = \sin x$.
  \[ \sin x = \sum_{n = 0}^{\infty} C_n x^n, \qquad C_n = \frac{f^{(n)}
     (0)}{n!} \]
  \begin{eqnarray*}
    \sin' x & = & \cos x\\
    \cos' x & = & - \sin x\\
    - \sin' x & = & - \cos x\\
    - \cos' x & = & \sin x
  \end{eqnarray*}
  substituting $x = 0$ we get a repeating mini series of $1, 0, - 1, 0,
  \ldots$. If we plug the coefficients we get:
  \[ \sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \cdots \]
  Substituting $z \in \mathbb{C}$ gives a convergent series which we define to
  be
  \[ \sin z = \sum_{\tmop{odd} \quad n} \frac{(- 1)^n_{\nosymbol} \cdot
     x^n}{n!} \]
  \item $f (x) = \cos x$.
  \begin{eqnarray*}
    \cos' x & = & f' (x) = - \sin x\\
    f'' (x) & = & - \cos x\\
    f^{(3)} (x) & = & \sin x\\
    f^{(4)} (x) & = & \cos x
  \end{eqnarray*}
  We get the Taylor series:
  \[ \cos x = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \cdots \]
  Substituting $z \in \mathbb{C}$ gives a convergent series which we define to
  be
  \[ \cos z = 1 - \frac{z^2}{2} + \frac{z^4}{4!} - \frac{z^6}{6!} + \cdots \]
  Differentiating our series for $\sin z$ term by term we obtain (just as in
  the real case) $\sin' z = \cos z, \tmxspace \cos' z = - \sin z$. So, if a
  function is represented by a series that converges {\tmem{uniformly}}, then
  its derivative can be obtained by differentiating the series term by term.
  
  \ 
\end{enumeratenumeric}
One of the properties of exponential functions is that using the relations
from \eqref{exponential-identities}, if we write $z = x + \mathi y$ ($x, y \in
\mathbb{R}$), then $\mathe^z = \mathe^x \cdot \mathe^{\mathi y}$. And by
definition:


\begin{eqnarray*}
  \mathe^{\mathi y} & = & \sum_{n = 0}^{\infty} \frac{(\mathi y)^n}{n!} =
  \tmcolor{red}{1} \tmcolor{blue}{+ \frac{\mathi y}{1}} \tmcolor{red}{-
  \frac{y^2}{2!}} \tmcolor{blue}{- \frac{\mathi y^3}{3!}} +
  \tmcolor{red}{\frac{y^4}{4!}} - \cdots\\
  & = & \tmcolor{red}{\left( 1 - \frac{y^2}{2!} + \frac{y^4}{4!} + \cdots
  \right)} + \tmcolor{blue}{\mathi \left( y - \frac{y^3}{3!} + \frac{y^5}{5!}
  \right)}\\
  & = & \tmcolor{red}{\cos y} + \tmcolor{blue}{\mathi \sin y}
\end{eqnarray*}
We just got Euler's formula! Additional useful relations:

\begin{tmframed}
  \[ \mathe^z = \mathe^x (\cos y + \mathi \sin y) \]
  \[ z = r (\cos \theta + \mathi \sin \theta) = r \cdot \mathe^{\mathi \theta}
  \]
\end{tmframed}

{\underline{Notes:}}
\begin{itemizedot}
  \item If $f$ is infinitely differentiable (real or analytic complex) then we
  have a unique Taylor series at 0, so any power series representation for $f$
  will be an alternative form of the Taylor series.
  
  \item Taylor series converge uniformly so if $f$ is infinitely
  differentiable then so is $f'$, therefore it also has a Taylor series. In
  other words
  \[ \tmop{if} \tmxspace f (x) = \sum C_n x^n \tmxspace \tmop{then} \tmxspace
     f' (x) = \sum_{n = 0}^{\infty} n C_n x^{n - 1} \]
  is the Taylor series for $f'$. Similarly, if $f$ is {\tmem{integrable}}, we
  get
  \[ \int f (x) \mathd x = \sum C_n  \frac{x^{n + 1}}{n + 1} \]
  is the Taylor series for $\int f (x) \mathd x$.
  
  \ 
\end{itemizedot}
{\underline{Example}}
\begin{enumerate}
  \item We want to calculate the Taylor series for $\arctan (x)$ at $x = 0$.
  \[ \arctan x = \sum_{n = 0}^{\infty} \frac{\arctan^{(n)} (0)}{n!} x^n \]
  Calculating directly we get:
  \begin{eqnarray*}
    \arctan (0) & = & 0\\
    \arctan' (x) & = & \frac{1}{x^2 + 1} ; \quad \arctan' (0) = 1\\
    \arctan'' (x) & = & \frac{- 2 x}{(x^2 + 1)^2} ; \quad \arctan'' (0) = 0
  \end{eqnarray*}
  Further derivatives become more difficult to calculate. Fret naught, there
  is a shortcut! If we find a power series that fits in some form, we know it
  is actually {\tmem{the}} Taylor series. If we look at
  \begin{eqnarray*}
    (1 - y + y^2 - y^3 + y^4 + \cdots) (1 + y) & = & \\
    = (1 - y + y^2 - y^3 + \cdots +) + (y - y^2 + y^3 - y^4 + \cdots +) & = &
    1
  \end{eqnarray*}
  So
  \[ 1 - y + y^2 - y^3 + \cdots = \frac{1}{1 + y} \tmxspace \quad \tmop{for}
     \quad | y | < 1 \]
  So
  \[ \frac{1}{1 + x^2} = 1 - x^2 + x^4 - x^6 + x^8 + \cdots \quad \tmop{for}
     \quad | x | < 1 \]
  The LHS is the derivative of $\arctan (x)$ so the RHS is its Taylor series
  at $x = 0$.
  
  Integrating the RHS term by term we get:
  \[ \arctan x = c + x - \frac{x^3}{3} + \frac{x^5}{5} - \frac{x^7}{7} +
     \frac{x^9}{9} + \cdots \]
  but $c = 0$, as can be seen by substituting values.
  
  Note that $\arctan 1 = \frac{\mathpi}{4}$. Plugging in the formula above we
  get ${\frac{\mathpi}{4} = 1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} +
  \frac{1}{9} + \cdots}$, which is called Leibniz's series. Actually, we
  observe convergence at $x \pm 1$. (There is no convergence for $| x | > 1$.)
  
  \item Taylor series for $\ln | 1 + x |$, defined for $x \neq - 1$.
  \[ (\ln | 1 + x |)' = \frac{1}{1 + x} = 1 - x + x^2 - x^3 + \cdots +
     \tmxspace \quad \tmop{for} \quad | x | < 1 \]
  Integrating term by term we get:
  \begin{eqnarray*}
    \ln | 1 + x | & = & c + x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4}
    + \cdots +
  \end{eqnarray*}
  Setting $x = 0$ we get $c = 0$.
  \[ \ln | 1 + x | = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} +
     \cdots + \tmxspace \quad \tmop{for} \quad | x | < 1 \]
  Note that the LHS is undefined at $x = - 1$ and the RHS equals ${- 1 -
  \frac{1}{2} - \frac{1}{3} - \frac{1}{4} - \cdots -}$ which diverges! But at
  $x = 1$ it {\tmem{does}} converge, as in fact: {{$\ln 2 = 1 - \frac{1}{2} +
  \frac{1}{3} - \frac{1}{4} + \cdots +$}.}
\end{enumerate}
\begin{tmornamented}
  {\underline{Reminder: Mathematical induction}}
  \begin{enumeratenumeric}
    \item If a claim dependent on a positive integer $n$ is true for $n = 1$
    
    \item If it is true for $k$ then it is true for $k + 1$
  \end{enumeratenumeric}
  Then the claim is true for all $n$.
\end{tmornamented}

\part*{Differential equations}

Informally, a differential equation (DE) is a functional equation which
involves functions, variables and derivatives of the functions. Note that the
solutions to differential equations will always be functions. An example:
\[ 2 y - y' = 0, \quad y = y (x) \]
\[ y' = 2 y \]
\[ y = \mathe^{2 x} \nospace \nospace \text{ is a solution.} \]
In fact, $y = ce^{2 x}$ is a solution for any $c \in \mathbb{R}$.

{\underline{Example: Radioactive decay}}

If a function $Q (t)$ is the amount of radioactive material at time $t$, then
the rate of decay is proportional to the amount present (at time $t$). By
these means, let there be $k \in \mathbb{R}$ such that
\[ Q' (t) = - kQ (t), \quad k > 0 \]
Note that $Q (t) = c \mathe^{- kt}$ solves the equation for any $c$. We show
these functions are the {\tmem{only}} solutions. Assuming $Q (t) > 0$ we can
write:
\[ \frac{Q' (t)}{Q (t)} = - k \]
The LHS is the logarithmic derivative of $\ln Q (t)$. Therefore, by
integration,
\[ \ln Q (t) = - kt + c \]
and after exponentiation,
\[ Q (t) = \mathe^{- kt + c} = \overbrace{\mathe^c}^{c'} \cdot \mathe^{- kt} .
\]
Removing the assumption that $Q (t) > 0$ we see this solves out equation for
any $c \in \mathbb{R}$.

To solve the equation for a {\tmem{specific}} material, {\tmem{a unique
solution}}, we need extra information such as the amount of material at $t =
0$, the {\tmem{initial condition}}.

{\underline{A numerical example}}

Thorium 234 decays at a rate such that 100 mg decays in a week to 82.04 mg.
What is the amount at time $t$?
\[ f' (t) = - kf (t) . \]
We know $f (t) = c \mathe^{- kt}$. Given $f (0) = 100 \tmop{mg}$ and $f (7) =
82.04 \tmop{mg}$, we can calculate both $c$ and $k$.
\begin{eqnarray*}
  100 & = & f (0) = c\\
  82.04 = f (7) & = & 100 \mathe^{- 7 k}\\
  \ln 0.8204 & = & - 7 k\\
  k & \approx & 0.02828 \tmop{days} .
\end{eqnarray*}
\[ \Rightarrow f (t) = 100 \mathe^{- 0.2828 t} \]
Differential equations can be complicated like $2 y - y'' \cdot x + y' \cdot
x^2 + 7 \sin y = 0$.

The {\tmem{order}} of a differential equation is the highest order of the
derivative that appears.

There are also {\tmem{partial differential equations}}, like Laplace's
equation:
\[ \frac{\partial^2 f (x, y)}{\partial x^2} + \frac{\partial^2 f (x,
   y)}{\partial y^2} = 0 \]

\section*{Definitions}

{\underline{Ordinary differential equations}}

{\tmem{Definition:}} An ordinary differential equation (ODE) is an equation of
the form:
\[ F (x, y, y', y'', \ldots, y^{(n)}) = 0 \]
where $F$ is some function of $n + 2$ variables.

{\underline{Partial differential equations}}

{\tmem{Definition:}} A partial differential equation (PDE) is one of the form:
\[ F \left( x, \ldots, x_k, y, \frac{\partial f}{\partial x}, \frac{\partial^2
   f}{\partial x_i \partial x_j}, \ldots \right) = 0 \]
where a solution would be $y = y (x, \ldots, x_k)$, which when substituted
with its partial derivatives, solves the equation.

{\underline{More examples of modeling processes with DEs}}


\begin{enumerate}
  \item A falling object:
  
  \begin{center}
    \raisebox{-0.490667057324418\height}{\includegraphics[width=4.94313262495081cm,height=5.93242489833399cm]{differential-equations-for-chemists-7.pdf}}
  \end{center}
  
  Define $v (t)$ the velocity of mass $m$, the drag force is proportionate to
  $v (t)$.
  
  By Newton's 2nd law, $F = ma$.
  \[ a (t) = v' (t) \]
  \begin{tmframed}
    \[ m \frac{\mathd v}{\mathd t} = mg - \gamma v \]
  \end{tmframed}
  
  Numerical example: $m = \SI{1}{\tmop{kg}}$, $\gamma =
  \SI{0.2}{\tmop{kg}}{s^{- 1}}$,
  \[ \frac{\mathd v}{\mathd t} = 9.8 - \frac{v}{5} \]
  Note that if you have some values of the function, you can plot them and get
  an approximation of the graph of the function and guess what the function
  (solution to DE) could be.
  
  \marginpar{lec 3 13.11.22}
  
  \begin{center}
    \begin{tabular}{|c|c|c|}
      \hline
      $t$ & $v (t)$ & $v' (t)$\\
      \hline
      0 & 40 & 1.8\\
      \hline
      arb & 40 & 1.8\\
      \hline
      & 50 & -0.2\\
      \hline
      & 30 & 3.8\\
      \hline
      & 49 & 0\\
      \hline
    \end{tabular}
    
    \ 
  \end{center}
  
  \begin{center}
    \raisebox{-0.5\height}{\includegraphics[width=14.8909550045914cm,height=8.92450478814115cm]{differential-equations-for-chemists-8.pdf}}
  \end{center}
  
  The solutions or {\tmem{curves}} that pass through a given point in the
  plane are called {\tmem{integral curves}}.
  
  Note that if $v' (t) = 0$ then the function doesn't change, which is a state
  of equilibrium. Here, the equilibrium is $v (t) \equiv 49 \quad \forall t$.
  In this case, the drag exactly matches the force of gravity. The velocity
  that satisfies this condition is called the {\tmem{limit velocity}}.
  
  \item Predator/prey = owls + field mice.
  
  if no predators assume rate of growth of mouse population, $r$, $p (t)$ is
  proportionate to the population level at that time.
  \[ \frac{\mathd p}{\mathd t} = r \cdot p (t) \]
  Now assume that we have owls who kill a fixed number of mice per month (for
  example 450) and that $r = 0.5 / \tmop{month}$, then
  \[ \frac{\mathd p}{\mathd t} = 0.5 p - 450 \]
  What we get is an {\tmem{unstable equilibrium}}. for $p (0) = 901$, i.e.,
  one mouse survives per month, the population will increase exponentially.
  for $p (0) = 899$ the mice population goes extinct.
  
  Equilibrium is unstable if $y (t) \rightarrow 0$ for $t \rightarrow \infty$
  if $y (0) < y_{\tmop{eq}}$.
\end{enumerate}
Let's solve the ODEs from examples 1 and 2 algebraically. Both are special
cases of
\[ y' = \frac{\mathd y}{\mathd t} = ay + b \]
Assume that $a \neq 0$ and that $y \neq - \frac{b}{a}$ (native equilibrium).
Divide by $a$:
\[ \frac{y'}{a} = y + \frac{b}{a} \]
\[ \frac{y'}{y + \frac{b}{a}} = a \]
This is the derivative of $\ln \left| y + \frac{b}{a} \right|$. Integrate both
sides to get
\[ \ln \left| y + \frac{b}{a} \right| = \tmop{at} + c \]
exponentiate both sides to get
\[ \left| y + \frac{b}{a} \right| = \mathe^{at + c} = \overbrace{\mathe^c}^C
   \cdot \mathe^{at} \]
an exponent is always positive so we can remove the absolute value by adding a
$\pm$ sign.
\[ y + \frac{b}{a} = \pm C \cdot \mathe^{at}, \quad C \in \mathbb{R} \]
our general solution is

\begin{tmornamented}
  \begin{equation}
    y = k \mathe^{\tmop{at}} - \frac{b}{a}, \quad k \neq 0, \nospace k \in
    \mathbb{R} \label{first-order-const-ode}
  \end{equation}
\end{tmornamented}

Note that $y \equiv - b / a$ is also a solution --- a specific solution.

Note that:
\begin{itemizedot}
  \item  if $a < 0$ then as $t \rightarrow \infty$ $y \rightarrow \frac{b}{a}$
  and we get a stable equilibrium.
  
  \item if $a > 0$ then:
  \begin{itemizeminus}
    \item if $k > 0$ then $y \rightarrow \infty$
    
    \item if $k < 0$ then $y \rightarrow - \infty$
  \end{itemizeminus}
  and the equilibrium is unstable.
\end{itemizedot}
What is the meaning of $k$? Given initial condition $y (0) = y_0$, then
setting $t = 0$ in eq. (\ref{first-order-const-ode}) gives: $y_0 = y (0) = k +
\frac{b}{a}$. So $k = y_0 - \frac{b}{a}$ is the unique solution satisfying the
initial condition (IC): $y = \left( y_0 - \frac{b}{a} \right) \mathe^{at} +
\frac{b}{a}$.

\part{First order ODEs}

First order ODEs are of the form
\[ y' = F (t, y) \]
The relationship between $t, y$ can be very complex! Let's review some kinds:

\section{Linear ODEs}

Linear in $y$, that is.
\[ y' = a (t) y + b (t) \]
In the previous examples we solved cases where $a (t)$ and $b (t)$ are
constants.

A method for solving linear ODEs:

\subsection{Integrating factors method: (due to Leibniz)}

Take for example the ODE $y' = - 2 y + 3$ ($a (t) = - 2, \nospace b (t) = 3$).

Isolate $f (y)$. rewrite as
\[ y' + 2 y = 3. \]
Now multiply by a function $\mu (t)$ so that LHS is recognizable as the
derivative of something, which is a {\tmem{product}}. Then we can integrate
both sides to get the solution. We have:
\[ (y \cdot \mu (t))' = \underline{y' \cdot \mu + y \cdot \mu'} \]
If we multiply out equation by $\mu (t)$,
\[ \underline{y' \cdot \mu (t) + 2 \mu (t) \cdot y} = 3 \cdot \mu (t) \]
All we need is that $\mu' (t) = 2 \mu (t)$. We may choose a solution
\[ \mu (t) = \mathe^{2 t} \]
and get
\[ y' \cdot e^{2 t} = 2 \mathe^{2 t} \cdot y = 3 \mathe^{2 t} \]
Integrate to get
\[ y \mathe^{2 t} = \int 3 \mathe^{2 t} \mathd t = \frac{3}{2} \mathe^{2 t} +
   c \]
so
\[ y = \frac{3}{2} + c \cdot e^{- 2 t} \]
{\tmstrong{Note that this method also works for non-linear ODEs}}.

{\underline{Another example}}
\[ y' + ay = b (t) \]
where $a$ is a constant and $b (t)$ isn't necessarily a constant. Multiply by
$\mu (t)$.
\[ y' \mu (t) + ay \mu (t) = b (t) \mu (t) \]
we need $\mu (t)$ such that $a \mu (t) = \mu' (t)$, so choose $\mu =
\mathe^{at}$ and get
\[ (y \cdot \mathe^{at})' = y' \cdot \mathe^{at} + ay \mathe^{at} = b (t)
   \mathe^{at} \]
Integrate both sides:
\[ y \cdot \mathe^{at} = \int b (t) \cdot \mathe^{at} \mathd t + c \]
\begin{tmornamented}
  \[ y = \mathe^{- at} \left[ \int b (t) \cdot \mathe^{at} \mathd t + c
     \right] \]
\end{tmornamented}

Consider the equation
\[ y' + 0.5 y = 2 + t ; \quad y (0) = 2 \]
Choose $\mu (t) = \mathe^{\frac{1}{2} t}$.
\[ {\color[HTML]{B8B8B8}\left( y \cdot \mathe^{\frac{1}{2} t} \right) =} y'
   \cdot \mathe^{\frac{1}{2} t} + \frac{1}{2} \mathe^{\frac{1}{2} t} y = 2
   \mathe^{\frac{1}{2} t} + t \mathe^{\frac{1}{2} t} \]
\[ y \mathe^{\frac{1}{2} t} = \int 2 \mathe^{\frac{1}{2} t} \mathd t + \int t
   \mathe^{\frac{1}{2} t} \mathd t \]
The second integral must be calculated via {\tmem{integration by parts}}.

\begin{tmornamented}
  \begin{eqnarray*}
    (uv)' & = & u' v + uv'\\
    uv & = & \int u' v \mathd t + \int uv' \mathd t\\
    \int u' v \mathd t & = & uv - \int uv' \mathd t\\
    \int v \mathd u & = & uv - \int u \mathd v
  \end{eqnarray*}
  
\end{tmornamented}

take $t = v, \nospace u' = \mathe^{\frac{1}{2} t}$, then $v' = 1, \nospace u =
2 \mathe^{\frac{1}{2} t}$.
\[ \int t \mathe^{\frac{1}{2} t} \mathd t = 2 t \mathe^{\frac{1}{2} t} - \int
   2 \mathe^{\frac{1}{2} t} \mathd t = 2 t \mathe^{\frac{1}{2} t} - 4
   \mathe^{t / 2} + c \]
So, in total,
\[ y \mathe^{\frac{1}{2} t} = 2 t \mathe^{t / 2} + c \]
\[ y = 2 t + c \cdot \mathe^{- t / 2} \]
Plug in $y (0) = 2$, which gives
\[ 2 = y (0) = 2 \cdot 0 + c \mathe^0 = c \]
We get the unique solution $y = 2 t + 2 \mathe^{- t / 2}$.

{\underline{Final example: the general case}}

For $y' = a (t) y + b (t)$, multiply through $\mu (t)$:
\[ y' \mu + a (t) y \mu = b (t) \mu \]
We want $y' \mu + a (t) y \mu = y \mu'$, or equivalently $a (t) \cdot \mu =
\mu'$. If $\mu \neq 0$:
\[ a (t) = \frac{\mu'}{\mu} = (\ln | \mu (t) |)' \rightarrow \int a (t) \mathd
   t = \ln | \mu (t) | \]
choose $\mu (t) = \mathe^{\int a (t) \mathd t}$ as int. factor.
\begin{eqnarray*}
  y' \mathe^{\int a (t) \mathd t} + ya (t) \mathe^{\int a (t) \mathd t} & = &
  b (t) \mathe^{\int a (t) \mathd t}\\
  & = & \left( y \cdot \mathe^{\int a (t) \mathd t} \right)'
\end{eqnarray*}
so that
\[ y \mathe^{\int a (t) \mathd t} = \int b (t) \mathe^{\int a (t) \mathd t} \]
\begin{tmornamented}
  \[ y = \mathe^{- \int a (t) \mathd t} \cdot \left[ \int b (t) \mathe^{\int a
     (t) \mathd t} \right] \]
\end{tmornamented}

{\underline{Example}}
\[ ty' + 2 y = 4 t^2, \quad t \neq 0, \quad y (1) = 2 \]
Rewrite as $y' + \frac{2}{t} y = 4 t$, such that $a (t) = \frac{2}{t}$. Choose
$\mu (t) = \mathe^{\int a (t) \mathd t} = \mathe^{2 \ln | t |} = t^2$.
\[ {\color[HTML]{B8B8B8}(t^2 y)' =} t^2 y' + 2 ty = 4 t^3 \]
Integrate to obtain
\[ t^2 y = \int 4 t^3 \mathd t = t^4 + c \]
\[ y = t^2 + \frac{c}{t^2} \]
Set $y (1) = 2$.
\[ 2 = 1 + \frac{c}{1} \rightarrow c = 1 \]
The unique solution is $y = t^2 + t^{- 2}$.

Question: Is there a solution for $t = 0$? Input $t = 0$ in the original DE:
\[ 0 \cdot y' + 2 y = 0 \rightarrow y (0) = 0 \]
First we see that $y (0)$ is \tmtextit{defined}. Then, we can see that if we
choose $c = 0$ we get a specific solution $y = t^2$.

\section{Separable ODEs}

Equations of the form (in Leibniz notation):
\[ \frac{\mathd y}{\mathd x} = \frac{M (x)}{N (y)} \]
for $M (x), N (y)$ functions only of $x, y$ respectively. Sometimes written
as:
\[ M (x) \mathd x - N (y) \mathd y = 0 \]
{\underline{Example}}
\[ \frac{\mathd y}{\mathd x} = \frac{x^2}{1 - y^2}, \quad y \neq \pm 1 \]
\[ \frac{\mathd y}{\mathd x} \cdot (1 - y^2) = x^2 \]
LHS is actually the derivative of $y - \frac{y^3}{3}$ with respect to $x$!
Integrating both sides gives
\[ y - \frac{y^3}{3} = \int x^2 \mathd x = \frac{x^3}{3} + c \]
So that $y$ is given implicitly.

\marginpar{lec 4 14.11.22}Note that if $N$ is a function of $y$ and $y$ is a
function of $x$. Suppose $\int N (y) \mathd y = Q (y)$ so $\frac{\mathd
Q}{\mathd y} = N (y)$, then, according to the chain rule,
\[ \frac{\mathd Q (y)}{\mathd x} = \frac{\mathd Q (y)}{\mathd y} \cdot
   \frac{\mathd y}{\mathd x} = N (y) \cdot y' \]
By taking inverse operation (integrating with respect to $x$) we find that
\[ \int N (y) \cdot y' \mathd x = Q (y) = \int N (y) \mathd y \]
This allows us to write $y' \mathd x = \mathd y$.

Let's solve the previous example using this notation.
\[ \frac{\mathd y}{\mathd x} = \frac{x^2}{1 - y^2} \]
\[ \int (1 - y^2) \mathd y = \int x^2 \mathd x \]
\[ y - \frac{y^3}{3} = \frac{x^3}{3} + c \]
When solved before, we had: $y' (1 - y^2) = x^2$ and integrated both sides
with respect to (wrt) $x$. What we did here is integrating wrt $y$ on the LHS
and wrt $x$ on the RHS. For separable ODEs, non-uniform integration is
justified.
\[ \  \]
{\underline{Example 2}}
\[ y' = \frac{3 x^2 + 4 x + 2}{2 (y - 1)} ; \quad y (0) = - 1, \quad y \neq 1
\]
\[ \int 2 (y - 1) \mathd y = \int (3 x^2 + 4 x + 2) \mathd x \]
\[ y^2 - 2 y = x^3 + 2 x^2 + 2 x + c \]
Last time we stopped at an implicit expression for $y$, but here we can go
further by applying the initial condition. Set $x = 0$: $1 + 2 = c$, so $c =
3$. We can write a quadratic expression in $y$ to get a more explicit
expression:
\[ y^2 - 2 y - (x^3 + 2 x^2 + 2 x + 3) = 0 \]
Solve using the quadratic formula:
\[ y = \frac{2 \pm \sqrt{4 + 4 (x^3 + 2 x^2 + 2 x + 3)}}{2} \]
\[ y = 1 \pm \sqrt{x^3 + 2 x^2 + 2 x + 4} \]
On paper, it seems now that applying the initial condition doesn't give us a
unique solution, because $y$ has two possible values! However, we shall see
that only one quadratic root satisfies the initial condition $y (0) = - 1$.

Substituting $x = 0$: $y (0) = 1 \pm \sqrt{4} = \left\{\begin{array}{l}
  3\\
  \tmcolor{blue}{- 1}
\end{array}\right.$. The unique solution is {$y = - 1 - \sqrt{x^3 + 2 x^2 + 2
x + 4} {}$}.

What is its domain of definition? The solution must (a) solve the ODE, (2)
satisfy the IC, and (3) be defined in some domain. We need that $\sqrt{x^3 + 2
x^2 + 2 x + 4} \geq 0$.
\[ x^3 + 2 x^2 + 2 x + 4 = (x + 2) (x^2 + 2) \]
So $\sqrt{x^3 + 2 x^2 + 2 x + 4} \geq 0$ for $x \geq - 2$.

\tmtextbf{But,} we defined the DE for $y \neq 1$, so we need $x^3 + 2 x^2 + 2
x + 4 > 0$, so $x > - 2$ is the true domain.

{\underline{Example 3}}
\[ \frac{\mathd y}{\mathd x} = \frac{x^2}{\underbrace{y (1 +
   x^3)_{\nosymbol}}_{\neq 0}} \]
\[ \frac{y^2}{2} = \int y \mathd y = \int \frac{x^2}{1 + x^3} \mathd x
   \overbrace{=}^{\tmscript{\begin{array}{c}
     u = 1 + x^3\\
     \mathd u = 3 x^2 \mathd x
   \end{array}}} \int \frac{1}{3 u} \mathd u = \frac{1}{3} \ln | 1 + x^3 | + c
\]
\[ y^2 = \frac{2}{3} \ln | 1 + x^3 | + c \]
Use the IC: $y (0) = 3$.
\[ 9 = \frac{2}{3} \ln 1 + c \rightarrow c = 9 \]
we get a unique solution (positive square root) $y = \sqrt{\frac{2}{3} \ln | 1
+ x^3 | + 9}$. The domain of the function is which satisfies $y > 0$. The
domain contains $x = 0$, so $x > - 1$.

{\underline{Example 4}}

Sometimes an equation can be {\tmem{reduced}} to a separable differential
equation by a change of variables. For example,
\[ y' = x^2 + 2 xy + y^2 = (x + y)^2 \]
We cannot separate the variables here, but we can define $z = x + y$,
substitute and get
\[ y' = z^2 \]
\[ \frac{\mathd z}{\mathd x} = 1 + y' = 1 + z^2 \]
the DE for $z$ \tmtextbf{is} separable. Get:
\[ \int \frac{\mathd z}{1 + z^2} = \int \mathd x \]
\[ \arctan z = x + c \]
Take tangent of both sides:
\[ z = x + y = \tan (x + c) \]
\[ y = \tan (x + c) - x \]
Cases we need to check to make sure we find {\tmstrong{all}} possible
solutions:

{\underline{Example:}}
\[ y' + y^2 \sin x = 0 \]
We want to divide by $y^2$ in order to solve, under the assumption for $y \neq
0$. Check some cases first:
\begin{enumeratenumeric}
  \item $y \equiv 0$. Is this a solution? --- Yes!
  
  Out solutions will be differentiable functions and so continuous. So that if
  $y \nequiv 0$ there is a point where $y \neq 0$ and around it there's an
  interval where $y (x) \neq 0$ for all $x$ in that interval. (This is because
  $y$ is continuous.)
  
  \item $y (x) \neq 0$ for all $x$ in an interval. We can divide by $y^2$ to
  get:
  \[ \frac{\mathd y}{y^2} + \sin x \mathd x = 0 \]
  \[ - \frac{\mathd y}{y^2} = \sin x \mathd x \]
  Integrate both sides:
  \[ - \frac{1}{y} = - \cos x + c \]
  \[ y = \frac{1}{c - \cos x} \]
  Note that this solution {\tmstrong{never}} has the value 0 for
  {\tmstrong{any}} $x$ where it is defined.
  
  If $| c | > 1$ then this holds for all $x$. Otherwise we need to avoid
  values where $\cos x = c$.
  
  What if $y (0) = - \frac{2}{3}$? we get $y (0) = \frac{1}{c - 1} \rightarrow
  c = - \frac{1}{2}$. That's problematic because $\cos x$ can get the value $-
  \frac{1}{2}$. So our solution holds in the interval $\left( -
  \frac{\mathpi}{3}, \frac{\mathpi}{3} \right)$, which contains $x = 0$.
  
  \item There exists a point $x_0$ where $y (x_0) = 0$ and $y \nequiv 0$. In
  that case, we solve as in case 2 for an interval where $y \neq 0$ for all
  $x$, as we got in case 2 solution.
  
  If we had a solution at $x_0$ where $y (x_0) = 0$, and in the interval $y$
  is the function in case 2, we cannot get a continuous extension of $y$ to
  $x_0$. In other words, this cannot happen!
  
  We've examined all possible cases, and thus found all the solutions.
\end{enumeratenumeric}
\marginpar{lec 5 20.11.22}{\underline{Another example of modeling with first
order DEs: Interest compounded continuously}}

$S (t) = \tmxspace$ amount of money deposited and interest evaluated
continuously, then we get that if the rate of change is proportionate to the
amount of money.
\[ S' (t) = r \cdot S (t) \]
In fact $r$ will be the annual rate of interest so that $S (t) = S (0)
\mathe^{rt}$. Why?

If we compute once a year then $S (1) = S (0) + rS (0) = S (0)  (1 + r)$.

If you do it twice a year: after 6 months we get $(0.5) = S (0)  \left( 1 +
\frac{r}{2} \right)$ and ${S (1) = S (0)  \left( 1 + \frac{r}{2} \right)^2}$,
and in general: $S (t) = S (0)  \left( 1 + \frac{r}{2} \right)^{2 t}$.

If we compute $n$ times a year: $S (t) = S (0)  \left( 1 + \frac{r}{n}
\right)^{nt}$.

Recall that $\lim_{n \rightarrow \infty} \left( 1 + \frac{r}{n} \right)^n =
\mathe^r$. So when $n \rightarrow \infty$ and interest is compounded
continuously we get $S (t) = S (0) \mathe^{rt}$.

Let's improve our model of money management. In addition to annual rate of
interest ($r$), we also deposit or withdraw $k$ amount of money every year.
Thus, the differential equation becomes
\[ S' (t) = rS (t) + k \]
As before, we get:
\begin{eqnarray*}
  S' (t) & = & \left( S_0 + \frac{k}{r} \right) \mathe^{rt} - \frac{k}{r}\\
  & = & \underbrace{S_0 \mathe^{rt}}_{\tmscript{\begin{array}{l}
    \text{\begin{tabular}{l}
      effect of the\\
      initial investment\\
      at interest rate r
    \end{tabular}}
  \end{array}}} + \underbrace{\frac{k}{r}  (\mathe^{rt} -
  1)}_{\tmscript{\begin{array}{l}
    \text{\begin{tabular}{l}
      result of the\\
      withdraws/deposits
    \end{tabular}}
  \end{array}}}
\end{eqnarray*}
{\underline{Example}}

Open a savings plan at age 25 with regular deposits of 2000\$/year with 8\%
annual rate of interest. What will be the amount saved by age 65?
\[ S (40) = S_0 \mathe^{rt} + \frac{k}{r} (\mathe^{rt} - 1), \quad S_0 = 0 \]
\[ S (40) = \frac{2000}{0.08}  (\mathe^{0.08 \ast 40} - 1) \approx \SI{588
   \comma 313}{\$} \]
We invested 80,000\$ and made a 508,000\$ profit! Note this is a special case
called an \tmtextit{autonomous equation}.

\subsection{Autonomous ODEs}

\tmtextbf{Definition:} An ODE is {\tmem{autonomous}} if the independent
variable does not appear explicitly.

For example, $y' = ay + b$, In general, this means $y' = F (y)$ so it's
separable.

When you plot a direction field for these ODEs you get replicas of vectors
along the horizontal axis.

{\underline{Example}}
\[ \frac{\mathd y}{\mathd x} = \frac{ay + b}{cy + e}, \qquad y \neq -
   \frac{e}{c} \]
There are 2 cases:
\begin{enumeratenumeric}
  \item $ay + b \equiv 0$, so $y \equiv - \frac{b}{a}$. A constant solution.
  
  (if $a = 0$ then $b = 0$ as well [because we can't divide by zero], and $y =
  k$ {\forall}$k \in \mathbb{R}$)
  
  \item Assume $ay + b \nequiv 0$, so take $ay + b \neq 0$ on some interval.
  We can now rewrite the equation to get the form: ($a \neq 0$)
  \[ \int \frac{cy + e}{ay + b} \mathd y = \int \mathd x = x + k \]
  Note that
  \begin{eqnarray*}
    \frac{cy + e}{ay + b} & = & \frac{\frac{c}{a} (ay + b) - \frac{c}{a} b +
    e}{ay + b}\\
    & = & \frac{c}{a} + \frac{- \frac{c}{a} b + e}{ay + b}\\
    & = & \frac{c}{a} + \frac{ae - bc}{a (ay + b)}
  \end{eqnarray*}
  Therefore,
  \[ \int \frac{cy + e}{ay + b} \mathd y = \int \frac{c}{a} \mathd y + \left(
     \frac{ae - bc}{a} \right)  \int \frac{\mathd y}{ay + b} = x + k \]
  We are left with an implicit expression for $y (x)$:
  \[ \  \]
  \[ \frac{c}{a} y + \left( \frac{ae - bc}{a} \right) \ln | ay + b | = x + k
  \]
  Note that if $a = 0$ we have to solve again from the start. Actually, in
  this case the equation is easier because we don't have $y$ in the
  denominator.
\end{enumeratenumeric}

\subsection{Verhulst's Model (1845)}

Similar to the field mice model, we had $y' = ry$ where $r$ was a constant
rate of population growth and $y (t)$ was the population at time $t$. We can
make the model more realistic by replacing $r$ by a function which depends on
$y$.
\[ y' = h (y) \cdot y \]
where $h (y)$ is the rate of population growth at population level $y$.

Typically, $h (y)$ {\tmem{decreases}} as $y$ {\tmem{increases}}. We
approximate this by choosing $h (y)$ to be linearly decreasing in $y$:
\[ h (y) = r - s \cdot y, \quad r, s > 0 \]
We get $y' = (r - sy) y$, a quadratic autonomous equation! Let's look at the
equilibria.

{\underline{Equilibrium points}}

\tmcolor{blue}{$y = 0$} or \tmcolor{blue}{$y = \frac{r}{s} = k$}. Rewrite our
ODE:
\[ y' = r \left( 1 - \frac{s}{r} \right) y = r \left( 1 - \frac{1}{k} y
   \right) y \]
Before we solve, let's look at the direction field. Take $r = 10, k = 5$:

$y' = 10 (1 - 5 y) = 10 y - 2 y^2$.

$\raisebox{-0.00177331370681444\height}{\includegraphics[width=6.94670733307097cm,height=4.73443854125672cm]{differential-equations-for-chemists-9.pdf}}$\raisebox{-0.00177331370681444\height}{\includegraphics[width=6.94670733307097cm,height=4.73443854125672cm]{differential-equations-for-chemists-10.pdf}}

\raisebox{-0.00177331370681444\height}{\includegraphics[width=6.94670733307097cm,height=4.73443854125672cm]{differential-equations-for-chemists-11.pdf}}\raisebox{-0.00177331370681444\height}{\includegraphics[width=6.94670733307097cm,height=4.73443854125672cm]{differential-equations-for-chemists-12.pdf}}

It is clear that $y \equiv 0$ is an unstable equilibrium and that $y \equiv 5$
is a stable equilibrium.

Now let's solve algebraically.
\[ y' = r \left( 1 - \frac{1}{k} y \right) y = r \left( \frac{k - y}{k}
   \right) y \]
If $y = k$ we get $y \equiv k$ (because $y' (k) = 0$. So now assume $y \neq k$
and separate the variables:
\[ \int \frac{k \mathd y}{(k - y) y} = \int r \mathd t = rt + c \]
As we have a polynomial at the denominator, we rewrite LHS using partial
fractions:
\[ \frac{k}{(k - y) y} = \frac{A}{k - y} + \frac{B}{y} \]
where $A, B$ are some constants. Get
\[ k = Ay + B (k - y) = (A - B) y + Bk \]
Equate coefficients on both sides:
\begin{eqnarray*}
  0 & = & A - B\\
  k & = & Bk
\end{eqnarray*}
We can see that $A = B = 1$.
\[ \int \frac{\mathd y}{k - y} + \int \frac{\mathd y}{y} = rt + c \]
\[ {\color[HTML]{A0A0A0}\ln \left( \left. \frac{| y |}{| k - y |} \right|
   \right) =} - \ln | k - y | + \ln | y | = rt + c \]
Exponentiate both sides:
\[ \frac{| y |}{| k - y |} = C \mathe^{rt} \]
Again, there are some cases:
\begin{enumeratenumeric}
  \item $0 < y < k$. Set $y (0) = y_0$.
  \[ \frac{y}{k - y} = C \mathe^{rt} \]
  Setting $t = 0$ to get
  \[ \frac{y_0}{k - y_0} = C \]
  Now we solve for $y$:
  \[ \frac{y}{k - y} = \frac{y_0}{k - y} \mathe^{rt} \]
  \[ y = (k - y)  \frac{y_0}{k - y_0} \mathe^{rt} \]
  \[ y \left( 1 + \frac{y_0}{k - y_0} \mathe^{rt} \right) = \frac{ky_0}{k -
     y_0} \mathe^{rt} \]
  \[ y = \frac{ky_0 \mathe^{rt}}{(k - y_0) \left[ 1 + \frac{y_0}{k - y_0}
     \mathe^{rt} \right]} = \frac{ky_0 \mathe^{rt}}{k - y_0 + y_0 \mathe^{rt}}
  \]
  Divide numerator and denominator by $\mathe^{rt}$.
  \[ y = \frac{ky_0}{(k - y_0) \mathe^{- rt} + y_0} \]
  As $t \rightarrow \infty$ we see that $y \rightarrow k$. This means that $y
  \equiv k$ is a stable equilibrium and that $y \equiv 0$ is an unstable
  equilibrium.
  
  \item if $y > k$ we get that case (1) solution is still valid ($y = k$ is a
  stable equilibrium).
\end{enumeratenumeric}
The constant $k = \frac{r}{s}$ is called the {\tmem{saturation level}} or
{\tmem{environmental carrying capacity}}.

\part{Second Order ODEs}

\section{Special 2nd order ODEs}

The solution be obtained by {\tmem{reducing}} to a \tmtextbf{first} order
equation. Two kinds of equations where you can do that:

\subsection{2nd order ODE where the dependent variable doesn't appear}

An equation of the form:
\[ y'' = F (x, y') \]
Substitute $v = y'$ and get the first order ODE
\[ v' = F (x, v) \]
and then solve
\[ y = \int v \mathd x \]
{\underline{Examples}}
\begin{enumeratenumeric}
  \item
  \[ t^2 y'' + 2 ty' - 1 = 0, \quad t > 0 \]
  Set $y' = v$:
  \[ t^2 v' + 2 tv = 1, \quad t > 0 \]
  Integrate both sides:
  \[ t^2 v = t + c_1 \]
  \[ {\color[HTML]{A0A0A0}y' =} v = \frac{1}{t} + \frac{c_1}{t^2} \]
  \[ y = \ln t - \frac{c_1}{t} + c_2 \]
  A second order ODE needs 2 initial conditions to get a unique solution.
  Given $y (0) = b_1$ and $y' (0) = b_2$ we can determine $c_1$ and $c_2$.
  
  \item
  \[ 2 t^2 y'' + (y')^3 = 2 y' t, \quad t > 0 \]
  Set $y' = v$:
  \[ 2 t^2 v' + v^3 = 2 vt \]
  Examine some cases:
  \begin{enumerateromancap}
    \item $v \equiv 0$. In that case $y \equiv C$. This solves the equation
    for any $C \in \mathbb{R}$.
    
    \item $v \neq 0$ on some interval. Divide by $v^3$ and get
    \[ \frac{2 t^2 v'}{v^3} + 1 = \frac{2 t}{v^2} \]
    \[ {\color[HTML]{A0A0A0}- \left( \frac{t^2}{v^2} \right)' =} \frac{2 t^2
       v'}{v^3} - \frac{2 t}{v^2} = - 1 \]
    Integrate both sides to get
    \[ \frac{t^2}{v^2} = t + c_1, \quad t + c_1 \neq 0 \]
    \[ {\color[HTML]{A0A0A0}(y')^2 =} v^2 = \frac{t^2}{t + c_1} \]
    \[ y' = v = \frac{\pm t}{\sqrt{t + c_1}}, \quad t + c_1 > 0 \]
    \[ y = \pm \int \frac{t \mathd t}{\sqrt{t + c_1}} \]
    Integrate by parts. $\int uv' = uv - \int vu'$. Choose $u = t, u' = 1$.
    
    $v' = \frac{1}{t + c_1}$ so $v = 2 \sqrt{t + c_1}$.
    \[ \Rightarrow y = \pm \left[ 2 t \sqrt{t + c_1} - 2 \int \sqrt{t + c_1}
       \mathd t \right] \]
    \begin{eqnarray*}
      y & = & \pm \left[ 2 t \sqrt{t + c_1} - 2 \cdot \frac{2}{3} (t + c_1)^{3
      / 2} + c_2 \right]\\
      & = & \pm \left[ \frac{2}{3}  \sqrt{t + c_1}  (t - 2 c_1) + c_2
      \right]\\
      &  & 
    \end{eqnarray*}
    Take initial conditions: $y (1) = 0, y' (1) = - 1$.
  \end{enumerateromancap}
  \begin{eqnarray*}
    y (1) = 0 & = & \frac{2}{3}  \sqrt{1 + c_1}  (1 - 2 c_1) + c_2\\
    y' (1) = - 1 & = & \frac{\pm 1}{\sqrt{1 + c_1}} \rightarrow c_1 = 0\\
    & \Rightarrow & c_2 = - \frac{2}{3}
  \end{eqnarray*}
  \[ y = - \frac{2}{3} t^{3 / 2} + \frac{2}{3}, \quad t > 0 \]
\end{enumeratenumeric}

\subsection{2nd order autonomous ODE}

The independent variable doesn't appear.
\[ y'' = F (y, y') \]
Set $y' = v$ and get
\[ v' = F (y, v) \]
But remember that $v' = \frac{\mathd v}{\mathd x}$. We want to express $v$ as
a function of $y$. Using the chain rule,
\[ y'' = v' = \frac{\mathd v}{\mathd x} = \frac{\mathd v}{\mathd y} \cdot
   \frac{\mathd y}{\mathd x} \]
Then we get an equation of the form:

\begin{tmornamented}
  \[ \frac{\mathd v}{\mathd y} \cdot v = F (y, v) \]
\end{tmornamented}

We got a first order ODE in $v$ as a function of $y$.

{\underline{Example}}
\[ y \cdot y'' + (y')^2 = 0 \]
Set $v = y'$:
\[ yv' + v^2 = 0 \]
\[ v' = \frac{\mathd v}{\mathd y} \cdot v \]
Rewrite as an ODE in $v$ as a function of $y$.
\[ y \cdot \frac{\mathd v}{\mathd y} \cdot v + v^2 = 0 \]
\begin{enumeratenumeric}
  \item $v \equiv 0$ gives $y' \equiv 0$ or $y \equiv C$ is a solution for all
  $C$.
  
  \item $v \neq 0$ on some interval. If there is a 2nd derivative it means the
  1st derivative is continuous, so if the 2nd derivative is non-zero at a
  point it is non-zero on the interval.
  \[ y \frac{\mathd v}{\mathd y} + v = 0 \]
  This equation is separable!
  \[ y \mathd v + v \mathd y = 0 \]
  \[ \int \frac{\mathd v}{v} + \int \frac{\mathd y}{y} = \int 0 \mathd t = C
  \]
  \[ \ln | vy | = \ln | v | + \ln | y | = C \]
  \[ \pm \mathe^C = k = vy \]
  Remember $v = \mathd y / \mathd x$ so we get
  \[ k = \frac{\mathd y}{\mathd x} \cdot y \]
  \[ k \cdot x + c = \int k \mathd x = \int y \mathd y = \frac{y^2}{2} \]
\end{enumeratenumeric}
\marginpar{lec 6 21.11.22}\section{Existence and uniqueness theorem for first
order ODEs}

Sometimes knowing that there exists a unique solution helps finding the
solution.

Suppose we have the first order ODE $y' = F (x, y)$, $y (a) = b$. If $F$ and
$\frac{\partial F}{\partial y}$ are continuous in some open rectangle around
$(a, b)$ in the $x$-$y$ plane, then there exists a unique solution to the ODE
satisfying $y (a) = b$.

Note that there is only one curve that passes through this curve and it's
defined in some open rectangle. The definition of an open rectangle is a set
of points $(x, y)$ such that {$\left\{ (x, y) | \begin{array}{l}
  a_1 < x < a_2\\
  b_1 < y, b_2
\end{array} \right\}$}.

Conditions guarantee that we cannot have a solution:

\begin{center}
  \raisebox{-0.190552269096248\height}{\includegraphics[width=9.53361537452447cm,height=4.93681949363768cm]{differential-equations-for-chemists-13.pdf}}
\end{center}

\begin{center}
  \raisebox{-0.0632682497801231\height}{\includegraphics[width=9.33528466483012cm,height=2.33052931916568cm]{differential-equations-for-chemists-14.pdf}}
\end{center}

{\underline{Examples}}
\begin{enumerate}
  \item $y' + 2 xy = x^3 y^2$
  \[ y' = x^3 y^2 - 2 xy = F (x, y) \quad \tmop{continuous}
     (\tmop{polynomial}) \]
  \[ \frac{\partial F}{\partial y} = 2 yx^3 - 2 x \quad \tmop{continuous}
     (\tmop{polynomial}) \]
  By the theorem, given any IC there exists a solution.
  
  \item First order linear: $y' = p (x) y + q (x) = F (x, y)$.
  
  $F$ is continuous $\Longleftrightarrow$ $p (x), q (x)$ are continuous.
  
  $\frac{\partial F}{\partial y} = p (x)$ is continuous $\Longleftrightarrow$
  $p (x)$ is continuous.
  
  Conclusion --- we have a unique solution for any IC if $p, q$ are both
  continuous.
  
  \item $y' = 2 \sqrt{y}$. Here $F (x, y)$ is cont. for all $x$ and for all $y
  > 0$ (and cont. on the right at $y = 0$).
  
  $\partial F / \partial y = \frac{1}{2 \sqrt{y}}$ is \tmtextbf{not} cont. at
  $y = 0$.
  
  What happens here when we have the IC: $y (0) = 0$ ? We solve assuming first
  that $y \neq 0$ (in some interval) in order to find a non-trivial solution
  that can be extended continuously to $y (0) = 0$.
  \[ {\color[HTML]{A0A0A0}\sqrt{y} =} \int \frac{\mathd y}{2 \sqrt{y}} = \int
     1 \mathd x = x + c \quad (x + c \geq 0) \]
  \[ y = (x + c)^2, \quad x \geq - c \]
  Notice that $(x + c) = 0$ is also a solution, therefore we can say that the
  interval is $x \geq c$ [The function is differentiable from the right].
  
  Let's draw the solutions. First notice that clearly $y \equiv 0$ is a
  solution satisfying IC.
  
  \begin{center}
    \raisebox{-0.164838769485289\height}{\includegraphics[width=12.2308474353929cm,height=3.44928177882723cm]{differential-equations-for-chemists-15.pdf}}
  \end{center}
  
  $y = x^2, (x \geq 0)$ is also a solution satisfying IC.
  
  Actually, we have 2 solutions for every IC such that $y (a) = 0$.
  
  Why could we extend the interval range to $x = - c$? Have a look at the
  following example:
  
  $y' \cdot y = \cos x \cdot y$. Notice that $y \equiv 0$ is a solution. Now
  assume that $y \nequiv 0$ and divide by an interval where $y (x) \neq 0$.
  \begin{eqnarray*}
    y' & = & \cos x\\
    y & = & \sin x + c
  \end{eqnarray*}
  Notice that $y = \sin x$ is a solution for $\sin x \neq 0, x \neq \mathpi
  k$.
  
  We then check and see that $y = \sin x$ is a solution \tmtextit{even} for $x
  = \mathpi k, k \in \mathbb{N}$.
  
  \item $y^2 + x^2 y' = 0$. Assuming $x, y \neq 0$ we can separate variables.
  \[ y^2 = - x^2 y' \]
  \[ - \int \frac{\mathd x}{x^2} = \int \frac{\mathd y}{y^2} \]
  \[ \frac{cx + 1}{x} = c + \frac{1}{x} = - \frac{1}{y} \]
  \[ y = - \frac{x}{cx + 1}, \quad \text{defined for all } x \neq -
     \frac{1}{c} \]
  Note that if $x = 0$ it is defined and then $y = 0$, so all solutions pass
  through $(0, 0)$.
  
  Given an IC $y (0) = 1$ --- there's no solution!
  
  But we do have a unique solution (by the E\&U theorem) for every IC $y (a) =
  b$ where $a \neq 0$, as $y' = F (x, y) = \frac{y^2}{x^2}$ is cont. for $x
  \neq 0$ and $\tfrac{\partial F}{\partial y} = \frac{2 y}{x}$ also cont. for
  $x \neq 0$.
  
  Also notice that there are inf. many solutions for $(0, 0)$ according to the
  solution $y = - \frac{x}{cx + 1}$.
  
  \tmtextbf{The E\&U theorem fails because $\partial F / \partial y$ is not
  continuous within the interval}.
  
  \item $xy' = 2 y$. $\Rightarrow$ $y' = \frac{2 y}{x}$ is linear. Set $p (x)
  = \frac{2}{x}$ cont. for $x \neq 0$.
  
  We solve assuming $x, y \neq 0$ and get
  \[ \frac{1}{2} \ln | y | = \int \frac{\mathd y}{2 y} = \int \frac{\mathd
     x}{x} = \ln | x | + c \]
  \[ \sqrt{y} = k | x | \]
  \[ y = Kx^2 \]
  This is a solution for all $K$, under the restriction $x \neq 0$.
  
  \begin{center}
    \raisebox{-0.405613964987638\height}{\includegraphics[width=6.26131444313262cm,height=5.25291879837334cm]{differential-equations-for-chemists-16.pdf}}
  \end{center}
  \begin{itemize}
    \item inf. many solutions satisfy $y (0) = 0$.
    
    \item No solution satisfies $y (0) = b$ where $b \neq 0$.
    
    \item There exists ($\exists$) a unique (!) solution for $y (a) = b$ for
    $a \neq 0$, any $b$.
    
    \item Notice that the combination of $y = - K_1 x^2, x < 0$ and $y = K_1
    x^2, x \geq 0$ \tmtextbf{is} differentiable at point $x = 0$
    \tmtextbf{and} solves the differential equation!
  \end{itemize}
\end{enumerate}

\subsection{Existence and uniqueness theorem for higher order ODEs}

Given a linear ODE of order $n$,
\[ y^{(n)} + p_{n - 1} (x) y^{(n - 1)} + p_{n - 2} (x) y^{(n - 2)} + \cdots +
   p_0 (x) y = q (x) \]
and the ICs:
\[ \left\{\begin{array}{l}
     y (a) = b_0\\
     y' (a) = b_1\\
     \vdots\\
     y^{(n - 1)} (a) = b_{n - 1}
   \end{array}\right. \]
If $p_0, p_1, \ldots, p_{n - 1}, q$ are continuous for all $x$ in some
interval containing $a$, then the ODE has a unique solution satisfying the ICs
for any choice of values $b_0, \ldots, b_{n - 1}$.

\section{Second order linear ODEs}

\subsection{Homogeneous ODEs}

Homogeneous equations take the general form:
\[ y'' + p (x) y' + q (x) y = 0 \]
By E\&U thm., for any IC $y (a) = b$, $y' (a) = c$ we have a unique sol. in
some neighborhood of $a$, provided $p, q$ are continuous in this interval.

\begin{tmornamented}
  {\underline{Reminder}}:
  \begin{itemize}
    \item The set $V = \left\{ f : \mathbb{R} \rightarrow \mathbb{R} | f
    \text{ is twice differentiable} \right\}$ is a {\tmem{vector space}} over
    $\mathbb{R}$ with respect to operations: addition of functions,
    multiplication by scalars from $\mathbb{R}$.
    
    Note: $V$ is \tmtextit{closed} under these operations. i.e., if $f, g \in
    V$ so are $f + g, c \cdot f$.
    
    Also note that we can define a vector space only in a confined interval
    $[x_1, x_2]$.
    
    {\underline{Note about dimension of $V$}}
    
    $\sin x, \cos x \in V$. $V$ is infinite dimensional over $\mathbb{R}$ as
    the monomial functions: $1, x, x^2, \ldots$,are linearly independent over
    $\mathbb{R}$. ($V$ can contain infinitely many linearly independent
    functions)
    
    \item in any vector space $V$: the elements $v_1, \ldots, v_k \in V$ are
    {\tmem{linearly independent}} if $\sum_{i = 1}^k a_i v_i = 0$ for $a_i$
    scalars only if $a_i = 0$ for all $i$.
    
    e.g. $\sin x, \cos x$ are linearly independent in our vector space of
    functions as
    \[ a \sin x + b \cos x \equiv 0 \]
    i.e.
    \[ a \sin x + b \cos x = 0 \quad \forall x \]
    Set $x = 0$ and get $b = 0$, and set $x = \mathpi / 2$ and get $a = 0$.
    
    \item A {\tmem{basis}} for $V$ is a set of linearly independent vectors
    such that every vector in $V$ is a linear combination of these vectors.
  \end{itemize}
  Turns out that:
  \begin{itemize}
    \item Every vector space has a basis (assuming axiom of choice).
    
    \item Every basis has the same cardinality (number of elements in the set
    if the set is finite).
    
    \item If $V$ has dimension $n$ then any linearly independent set of $n$
    elements will be a basis.
  \end{itemize}
\end{tmornamented}

{\underline{Claim}}: The set of solutions to our ODE
\begin{equation}
  y'' + p (x) y' + q (x) y = 0, \quad p, q \text{ continuous} \label{homo-2nd}
\end{equation}
is a vector space over $\mathbb{R}$ (a subspace of $V$). In fact, it is a
vector space of dimension 2.

{\underline{Proof:}} Set of solutions is non-empty as $y \equiv 0$ is a
solution.

We need to show that if $y_1, y_2$ are solutions, then so is $y_1 + y_2$ and
also $c \cdot y_1$ for $c \in \mathbb{R}$.

$y_1$ is a solution so:
\[ y_1'' + p (x) y'_1 + q (x) y_1 = 0, \quad \forall x \]
\[ y_2'' + p (x) y'_2 + q (x) y_2 = 0, \quad \forall x \]
So:
\[ \underbrace{(y_1'' + y_2'')}_{(y_1 + y_2)''} + p (x)  \underbrace{(y_1' +
   y_2')}_{(y_1 + y_2)'} + q (x)  (y_1 + y_2) = 0 \]
\[ \underbrace{(y_1'' + p (x) y_1' + q (x) y_1)}_{= 0} + \underbrace{(y_2'' +
   p (x) y_2' + q (x) y_2)}_{= 0} = 0 \]
So $y_1 + y_2$ is a solution and
\[ cy_1'' + cp (x) y'_1 + cq (x) y_1 = c \underbrace{(y_1'' + p (x) y'_1 + q
   (x) y_1)}_{= 0} = 0 \]
and so $cy_1$ is a solution.

{\underline{Conclude}}: Set of all solutions is a subspace of $V$.

It remains to show dimension of this subspace is 2. Here we use E\&U thm.
Given a point $a$ we have a solution such that $y (a) = 1$. There is at least
one solution, so the dimension must be $\geq 1$. [Dimension is zero if only $y
\equiv 0$ is a solution]

On the other hand, we have a solution $y_1$ such that $y_1 (a) = 1$ and $y_1'
(a) = 0$ and another solution $y_2$ such that $y_2 (a) = 0$ and $y_2' (a) =
1$.

if $y_1$ and $y_2$ were linearly dependent, we would have $\alpha \in
\mathbb{R}$ such that $y_2 = \alpha \cdot y$. But if we substitute $y_2 (a) =
\alpha y_1 (a) = \alpha$, we get $\alpha = 0$. But then $y_2 \equiv 0$, and we
know that $y' (a) = 1$ --- contradiction!

That means $y_1, y_2$ are linearly independent and the dimension of the
subspace is \tmtextbf{at least} 2. Now we only need to show that the dimension
is \tmtextbf{exactly} 2.

\marginpar{lec 7 27.11.22}Let $f (x)$ be an arbitrary solution to
\eqref{homo-2nd}, such that $f (a) = b$ and $f' (a) = c$. Now look at the
following function:
\[ g (x) = by_1 (x) + cy_2 (x) \]
$y_1 (x), y_2 (x)$ are solutions and therefore $g (x)$ also solves
\eqref{homo-2nd}.

Set $x = a$:
\[ g (a) = by_1 (a) + cy_2 (a) = b \]
\[ g' (a) = by_1' (a) + cy_2' (a) = c \]
So $g (x)$ solves the ODE and satisfies the same ICs as the arbitrary function
$f (x)$. But by the E\&U thm. there's only one solution satisfying a given set
of ICs, so $f (x) = g (x)$ for all $x$ and $f (x)$ is a linear combination of
$y_1$ and $y_2$.

To conclude, all solutions can be written as linear combinations of $y_1,
y_2$, meaning that the subspace is \tmtextit{spanned} by $y_1, y_2$, and its
dimension is exactly 2.

{\underline{Example Usage of E\&U thm.}}

Given the ODE
\[ y'' + y = 0 \]
Note that $\sin x$ and $\cos x$ are solutions. We want the unique sol. such
that
\[ \left\{\begin{array}{l}
     y (0) = 3\\
     y' (0) = - 2
   \end{array}\right. \]
$\sin x, \cos x$ are linearly independent (as if $\alpha \sin x + \beta \cos x
= 0$ for all $x$, \ setting $x = 0$ gives: $\alpha \sin 0 + \beta \cos 0 = 0
\rightarrow \beta = 0$, so $\alpha \sin x = 0$ for all $x$, so $\alpha = 0$.)

$\sin x, \cos x$ are therefore a basis for set of solutions. We want a
function $y (x) = a \sin x + b \cos x$ such that the ICs hold. We get
\[ 3 = y (0) = a \sin 0 + b \cos 0 = b \]
\[ - 2 = y' (0) = a \cos (0) = a \]
So $y (x) = - 2 \sin x + 3 \cos x$ is the unique solution. Knowing that a
solution exists and the E\&U thm. holds means that $y (x)$ is the
\tmtextbf{only} solution that satisfies the ICs.

\begin{tmornamented}
  {\underline{Note}}
  
  1st order homogeneous linear ODEs are of form: $y' + p (x) y = 0$. We solved
  and found that also solutions were multiples of $\mathe^{\int p (x) \mathd
  x}$, i.e. a 1-dimensional space of functions solves the ODE. We expect (and
  shall see later) that for an ODE of order $n$, the space of solutions would
  be $n$-dimensional.
\end{tmornamented}

\subsection{Finding a basis for the set of solutions}

\[ y'' + p (x) y' + q (x) y = 0 \]
If $p (x), q (x)$ are \tmtextbf{not} constant functions, this can be
difficult.

There are some special situations for which we can use certain tricks to
solve.

\subsubsection{When one solution is known}

Imagine we somehow know one non-zero solution and want to find a second
solution which is linearly independent.

Suppose $y_1$ solves our ODE. We want $y_2$ which is \tmtextbf{not} a multiple
of $y_1$. It means that $\frac{y_2}{y_1}$ is not a constant, but a function,
$v (x)$.

In other words, $v (x)$ is non-constant and $y_2 (x) = y_1 (x) \cdot v (x)$ is
a solution. We substitute $y_2$ in the ODE:
\[ y_2'' + p (x) y'_2 + q (x) y_2 = 0 \]
\begin{eqnarray*}
  y'_2 & = & y_1' v + y_1 v'\\
  y_2'' & = & y_1'' v + 2 y_1' v' + y_1 v''
\end{eqnarray*}
Get
\[ (\tmcolor{red}{y_1'' v} + \tmcolor{blue}{2 y_1' v'} + y_1 v'') + p (x) 
   (\tmcolor{red}{y_1' v} + \tmcolor{blue}{y_1 v'}) + q (x) 
   (\tmcolor{red}{y_1 v}) = 0 \]
Rewrite as
\[ \tmcolor{red}{(y_1'' + p (x) y'_1 + q (x) y_1)} v + \tmcolor{blue}{(2 y_1'
   + p (x) y_1)} v' + y_1 v'' = 0 \]
The \tmcolor{red}{red} term is equal to zero because $y_1$ is a solution.
Let's look at what's left.
\[ (2 y_1' + py_1) v' + y_1 v'' = 0 \]
This is a 2nd order ODE in $v$, or a 1st order linear ODE in $v'$. Therefore
we can find $y_2$ by ``reduction of order''.

{\underline{Examples}}
\begin{enumerate}
  \item $y + y'' = 0$. Suppose given $y_1 = \sin x$. Set $y_2 = y_1 v = \sin x
  \cdot v$.
  \begin{eqnarray*}
    y'_2 & = & \cos x \cdot v + \sin x \cdot v'\\
    y_2'' & = & - \sin x \cdot v + 2 \cos x \cdot v' + \sin x \cdot v''
  \end{eqnarray*}
  Substitute and get
  \[ (\tmcolor{red}{- \sin x \cdot v} + 2 \cos x \cdot v' + \sin x \cdot v'')
     + \tmcolor{red}{\sin x \cdot v} = 0 \]
  \[ v'' \cdot \sin x + 2 \cos x \cdot v' = 0 \]
  Set $v' = z$.
  \[ z' \cdot \sin x + 2 \cos x \cdot z = 0 \]
  This is a separable ODE.
  \[ - 2 \cos x \cdot z = \frac{\mathd z}{\mathd x} \cdot \sin x \]
  \[ - 2 \int \frac{\cos x}{\sin x} \mathd x = \int \frac{\mathd z}{z} \]
  \[ - 2 \ln | \sin x | = \ln | z | + c \]
  Take $c = 0$
  \[ \ln (\sin^{- 2} x) = \ln | z | \]
  \[ v' = z = \frac{1}{\sin^2 x} \Rightarrow v = \cot x \text{is a solution
     for $\sin x \neq 0$.} \]
  So out second solution is
  \[ y_2 = \sin x \cdot v (x) = \sin x \cdot \cot x = \cos x \]
  But we can verify $\cos x$ is a solution for all $x$, not only when $\sin x
  \neq 0$.
  
  \item $2 x^2 y'' + 3 xy' - y = 0, \quad x > 0$. By ``inspection'' we see
  that $y = \frac{1}{x}$ is a solution.
  \begin{eqnarray*}
    \left( \frac{1}{x} \right)' & = & - \frac{1}{x^2}\\
    \left( \frac{1}{x} \right)'' & = & \frac{2}{x^3}
  \end{eqnarray*}
  \[ \Rightarrow 2 x^2 \cdot \frac{2}{x^3} + 3 x \cdot \left( - \frac{1}{x^2}
     \right) - \frac{1}{x}  \overset{\checkmark}{=} 0 \]
  We want $v (x) \equiv C$ such that $y = \frac{v}{x}$ is a solution.
  \begin{eqnarray*}
    y' & = & \frac{v'}{x} - \frac{v}{x^2}\\
    y'' & = & \frac{v''}{x} - \frac{2 v'}{x^2} + \frac{2 v}{x^3}
  \end{eqnarray*}
  Substitute in the ODE:
  \[ 2 x^2  \left[ \frac{v''}{x} - \frac{2 v'}{x^2} + \frac{2 v}{x^3} \right]
     + 3 x \left[ \frac{v'}{x} - \frac{v}{x^2} \right] - \frac{v}{x} = 0 \]
  \[ 2 x^2 \cdot \frac{v''}{x} - v' = 0 \]
  Set $v' = z \nequiv 0$.
  \[ 2 x \cdot \frac{\mathd z}{\mathd x} = z \rightarrow \frac{\mathd z}{z} =
     \frac{\mathd x}{2 x} \]
  \[ \ln | z | = \ln \sqrt{x}, \quad x > 0 \]
  \[ v' = z = c \cdot x^{1 / 2} \]
  \[ v' = \frac{2}{3} c \cdot x^{3 / 2} \]
  Take $c = \frac{3}{2}$.
  \[ y = \frac{v}{x} = \frac{x^{3 / 2}}{x} = \sqrt{x} \]
  Out 2nd linearly independent solution will be $\sqrt{x}$ and the general
  solution to our ODE is $y = a \sqrt{x} + b \frac{1}{x}$.
  
  \item $y'' - 3 y' + 2 y = 0$. Note that $y = \mathe^x$ is a solution. We use
  reduction of order to find a second solution: $y = \mathe^x \cdot v (x)$.
  \begin{eqnarray*}
    y' & = & \mathe^x \cdot v + \mathe^x \cdot v' = \mathe^x (v + v')\\
    y'' & = & \mathe^x v + 2 \mathe^x v' + \mathe^x v'' = \mathe^x (v + 2 v' +
    v'')
  \end{eqnarray*}
  Substitute:
  \[ \mathe^x (v + 2 v' + v'') - 3 \mathe^x (v + v') + 2 \mathe^x v = 0 \]
  Divide by $\mathe^x \neq 0 \forall x$, rearrange and get:
  \[ v'' = v' \]
  $v = \mathe^x$ solves the problem, and $y = \mathe^{2 x}$ is a 2nd solution.
  
  Alternative approach: Suppose we start with $\mathe^{2 x}$ and want a 2nd
  solution of the form $y = v \mathe^{2 x}$.
  \begin{eqnarray*}
    y' & = & 2 \mathe^{2 x} v + \mathe^{2 x} v'\\
    y'' & = & 4 \mathe^{2 x} v + 4 \mathe^{2 x} v' + \mathe^{2 x} v''
  \end{eqnarray*}
  \[ (4 \mathe^{2 x} v + 4 \mathe^{2 x} v' + \mathe^{2 x} v'') - 3 (2
     \mathe^{2 x} v + \mathe^{2 x} v') + 2 \mathe^{2 x} v = 0 \]
  \[ \mathe^{2 x} v' + \mathe^{2 x} v'' = 0 \]
  \[ v'' = - v' \Rightarrow v (x) = - c \mathe^{- x}, \quad \text{take } c = -
     1. \]
  \[ y = \mathe^{2 x} \cdot \mathe^{- x} = \mathe^x \]
  We've got the solution from the first approach.
  
  \item $x^2 y'' - 5 xy + 9 y = 0$. Notice $y = x^3$ is a solution. Want a
  sol. of form $y = x^3 v$.
  \begin{eqnarray*}
    y' & = & 3 x^2 v + x^3 v'\\
    y'' & = & 6 xv + 6 x^2 v' + x^3 v''
  \end{eqnarray*}
  Get
  \[ x^2  (6 xv + 6 x^2 v' + x^3 v'') - 5 x (3 x^2 v + x^3 v') + 9 x^3 v = 0
  \]
  \[ x^4 v' + x^5 v'' = 0 \]
  Assuming $x \neq 0$, get
  \[ v' + xv'' = 0 \]
  Set $z = v'$, solve for $z$ and finally get $v (x) = \ln | x |$, $y = x^3
  \cdot \ln | x |$.
\end{enumerate}
\marginpar{lec 8 28.11.22}{\underline{Claim}}

Given a linear homogeneous ODE of order $n$,
\[ y^{(n)} + p_{n - 1} (x) y^{(n - 1)} + \cdots + p_1 (x) y' + p_0 (x) y = 0
\]
the set of solutions is a vector space of functions of dimension $n$.

{\underline{Proof}}

For $n = 2$ we've already proved. It is easy to see it is a subspace ---
showing dim. is $n$ is similar to case we did for $n = 2$.

\subsubsection{All coefficients are constants}

All coeffs are constants: $\forall k$: $p_k^{\nosymbol} (x) \equiv a_k \in
\mathbb{R}$.

Rewrite ODE as
\[ a_n y^{(n)} + a_{n - 1} y^{(n - 1)} + \cdots + a_0 y = 0, \quad a_n \neq 0
\]
{\underline{Example}}: $y'' - y' = 0$.

Clearly the $y \equiv 1$ and $y = \mathe^x$ are both solutions and are
linearly independent. Therefore, they form a basis for the space of solutions.
Given ICs $y (0) = 1, y' (0) = 0$, we can construct the general solution as a
linear combination of our basis:
\[ \tmop{General} \tmop{sol} . \nospace \quad y = a + b \mathe^x \]
\begin{eqnarray*}
  1 & = & y (0) = a + b\\
  0 & = & y' (0) = b
\end{eqnarray*}
So we get \ $a = 1, b = 0$ and the unique solution is $y \equiv 1$.

{\underline{Another example}}: $y'' - 2 y' - 15 y = 0$. It makes sense to
guess a solution of the form $y = \mathe^{\lambda x}$. Then:
\begin{eqnarray*}
  y' & = & \lambda \mathe^{\lambda x}\\
  y'' & = & \lambda^2 \mathe^{\lambda x}
\end{eqnarray*}
Substitute and get
\[ \lambda^2 \mathe^{\lambda x} - 2 \lambda \mathe^{\lambda x} - 15
   \mathe^{\lambda x} = 0 \]
\[ \mathe^{\lambda x}  (\lambda^2 - 2 \lambda - 15) = 0 \]
$\mathe^{\lambda x}$ is a solution $\Longleftrightarrow$ $\lambda^2 - 2
\lambda - 15 = 0$. Solve the quadratic equation:
\[ \frac{2 \pm \sqrt{4 + 60}}{2} = \frac{2 \pm 8}{2} = \left\{\begin{array}{l}
     5\\
     - 3
   \end{array}\right. \]
The solutions $\mathe^{- 3 x}$ and $\mathe^{5 x}$ are both linearly
independent solutions! So the general solution will be $y = a \mathe^{- 3 x} +
b \mathe^{5 x}$.

In the previous example, we would have gotten $y \equiv 1$ using the method
above by finding that $\lambda = 0$.

\begin{tmornamented}
  {\underline{General case}}
  
  Given: $a_n y^{(n)} + a_{n - 1} y^{(n - 1)} + \cdots + a_0 y = 0, \quad a_n
  \neq 0$ we look for solutions of the form $\mathe^{\lambda x}$ and get:
  \[ y^{(k)} (x) = \lambda^k \mathe^{\lambda x} \quad \forall x \]
  And when we substitute in the ODE:
  \[ \mathe^{\lambda x}  (a_n \lambda^n + a_{n - 1} \lambda^{n - 1} + \cdots +
     a_1 \lambda + a_0) = 0 \]
  Then $\mathe^{\lambda x}$ is a solution if and only if $a_n \lambda^n + a_{n
  - 1} \lambda^{n - 1} + \cdots + a_1 \lambda + a_0 = 0$. (This polynomial is
  also called the \tmtextit{characteristic polynomial of the ODE}.)
  
  If the polynomial has $n$ distinct (real of complex) solutions $\lambda_1,
  \ldots, \lambda_n$ then $\mathe^{\lambda_1 x}, \ldots, \mathe^{\lambda_n x}$
  are $n$ linearly independent solutions and form a basis for space of
  solutions.
\end{tmornamented}

{\underline{Fundamental thm. of Algebra (Gauss)}}

Every polynomial equation over $\mathbb{C}$ has $n$ solutions including
multiplicities.

{\underline{Solution of polynomial equations}}
\[ a_n \lambda^n + a_{n - 1} \lambda^{n - 1} + \cdots + a_1 \lambda + a_0 = 0
\]
There are \tmtextbf{no} formulas that give the roots of polynomials (in
general) for $n \geq 5$, in terms of the coefficients.

{\underline{Example: 3rd order ODE}}
\[ y^{(3)} - 6 y'' + 11 y' - 6 = 0 \]
A trick to solve cubic equations: if you can find one solution $\lambda_0$
then you get an equation of the form $(\lambda - \lambda_0) \left(
\text{quadratic equation} \right) = 0$. So check if ${\lambda = 0, \pm 1, \pm
2, \ldots}$ are solutions and the problem might be simplified.

In this case we need to solve
\[ \lambda^3 - 6 \lambda^2 + 11 \lambda - 6 = 0 \]
Setting $\lambda = 1$: $1 - 6 + 11 - 6
\overset{}{\overset{}{\overset{\checkmark}{=}}}^{\nosymbol} 0$.
\[ \lambda^3 - 6 \lambda^2 + 11 \lambda - 6 = (\lambda - 1)  (\lambda^2 - 5
   \lambda + 6) = (\lambda - 1)  (\lambda - 2)  (\lambda - 3) \]
The general solution is $y = a \mathe^x + b \mathe^{2 x} + c \mathe^{3 x}$.

\

{\underline{Example: The characteristic polynomial does not have $n$ distinct
roots}}

We still need $n$ linearly independent solutions. What do we do?
\[ y'' - 4 y' + 4 y = 0 \]
The characteristic polynomial is $\lambda^2 - 4 \lambda + 4 = (\lambda -
2)^2$. Our method yields only one exponential function: $\mathe^{2 x}$.

We look for a second linearly independent solution. We want a non-constant
function $v (x)$ such that $y = \mathe^{2 x} v (x)$ solves the ODE.
\begin{eqnarray*}
  y' & = & 2 \mathe^{2 x} v (x) + \mathe^{2 x} v' (x) = \mathe^{2 x}  (2 v +
  v')\\
  y'' &  & 2 \mathe^{2 x}  (2 v + v') + \mathe^{2 x}  (2 v' + v'') = \mathe^{2
  x}  (4 v + 4 v' + v'')
\end{eqnarray*}
\[ \not{\mathe^{2 x}}  (4 v + 4 v' + v'') - 4 \not{\mathe^{2 x}}  (2 v + v') +
   4 \not{\mathe^{2 x}} v = 0 \]
\[ v'' = 0 \Rightarrow v \text{ is linear in } x \]
So take $v = x$ and get  \{$\mathe^{2 x}, x \mathe^{2 x}$\}  is the basis for
set of solutions.

\begin{tmornamented}[roundcorner=0pt]
  {\underline{In general}}
  
  If $\lambda_0$ is a root of the characteristic polynomial of multiplicity
  $r$, then ${\mathe^{\lambda_0 x}, x \mathe^{\lambda_0 x}, x^2
  \mathe^{\lambda_0 x}, \ldots, x^{r - 1} \mathe^{\lambda_0 x}}$ will all
  solve the ODE and are linearly independent, and will be linearly independent
  of solutions we obtain from other roots of the characteristic polynomial.
\end{tmornamented}

{\underline{Example}}: Suppose we know that the char. poly. factors as
$\lambda^3  (\lambda + 3)^2  (\lambda - 1)^2$: A 7th order linear ODE.
\begin{itemize}
  \item From $\lambda = 0$ we get the solutions: $y = 1, x, x^2$.
  
  \item From $\lambda = 3$ we get the solutions: $y = \mathe^{- 3 x}, x
  \mathe^{- 3 x}$.
  
  \item From $\lambda = 1$ we get the solutions $y = \mathe^x, x \mathe^x$.
\end{itemize}
{\underline{Example: Roots of the characteristic polynomial are not real}}
\[ y'' + y = 0 \]
Our method yields the roots of char. poly. $x^2 + 1$: $\pm \mathi$. Get 2
non-real solutions: $\mathe^{\mathi x}, \mathe^{- \mathi x}$. These span the
vector space of all complex solutions.

We note that the span over $\mathbb{C}$ $\{ \mathe^{\mathi x}, \mathe^{-
\mathi x} \}$ is a space of dimension 2 of complex functions. We also have 2
real solutions: $\sin x$ and $\cos x$, that are linearly independent. This
means that the span can be written as $\{ \sin x, \cos x \}$. This makes
sense, as {$\mathe^{\mathi x} = \cos x + \mathi \sin x$, $\mathe^{- \mathi x}
= \cos x - \mathi \sin x$}, and $\frac{1}{2} (\mathe^{\mathi x} + \mathe^{-
\mathi x}) = \cos x, \frac{1}{2 \mathi} (\mathe^{\mathi x} - \mathe^{- \mathi
x}) = \sin x$.

$\mathe^{\mathi x}, \mathe^{- \mathi x}$ are in the span of $\{ \sin x, \cos x
\}$, and $\cos x, \sin x$ are in the span of $\{ \mathe^{\mathi x}, \mathe^{-
\mathi x} \}$. However, $\tmop{span}_{\mathbb{C}}  \{ \sin x, \cos x \}
\nonconverted{nsupset} \tmop{span}_{\mathbb{R}}  \{ \sin x, \cos x \}$.

{\underline{Note}}: if $z (x)$ is a complex-valued function solving our ODE,
then it can be written as:
\[ z (x) = u (x) + \mathi v (x), \quad u, v \in \mathbb{R} \]
Then we have
\[ (u + \mathi v)^{(n)} + a_{n - 1} (u + \mathi v)^{(n - 1)} + \cdots + a_1 (u
   + \mathi v)' + a_0 (u + \mathi v) = 0 \]
\[ u^{(n)} + \mathi v^{(n)} + a_{n - 1} (u^{(n - 1)} + \mathi v^{(n - 1)}) +
   \cdots + a_1 (u' + \mathi v') + a_0 (u + \mathi v) = 0 \]
\[ (u^{(n)} + a_{n - 1} u^{(n - 1)} + \cdots + a_1 u' + a_0 u) + \mathi
   (v^{(n)} + a_{n - 1} v^{(n - 1)} + \cdots + a_1 v' + a_0 v) = 0 \]
This holds for all $x$ if and only if:
\[ \left\{\begin{array}{l}
     u^{(n)} + a_{n - 1} u^{(n - 1)} + \cdots + a_1 u' + a_0 u = 0\\
     v^{(n)} + a_{n - 1} v^{(n - 1)} + \cdots + a_1 v' + a_0 v = 0
   \end{array}\right. \]
Therefore, $u (x), v (x)$ are \tmtextbf{both} real solutions to the ODE.

Note that if $z (x)$ is a complex solution to our ODE, then so is $\bar{z} (x)
= u (x) - \mathi v (x)$, as since we have:
\[ z^{(n)} + a_{n - 1} z^{(n - 1)} + \cdots + a_1 z' + a_0 z = 0 \]
Since $a_i \in \mathbb{R}$, $\bar{a}_i = a_i$. Because:
\[ \overline{z_1 \cdot z_2} = \overline{z_1}_{\nosymbol}  \overline{z_2} \]
\[ \overline{z_1 + z_2} = \overline{z_1} + \overline{z_2} \]
We get
\[ \bar{z}^{(n)} + a_{n - 1}  \bar{z}^{(n - 1)} + \cdots + a_1  \bar{z}' + a_0
   \bar{z} = 0 \]
In conclusion, \tmtextbf{complex roots come in pairs}. So every complex
solution $z (x)$ gives rise to another, $\bar{z} (x)$, and these give 2
linearly independent real solutions $u (x), v (x)$.

\marginpar{lec 9 04.12.22}{\underline{Example: Roots are complex, where the
real part is non-zero}}
\[ y'' + y' + y = 0 \]
Characteristic polynomial is $\lambda^2 + \lambda + 1$,
\[ \lambda^2 + \lambda + 1 = 0 \Longleftrightarrow \lambda = \frac{- 1 \pm
   \sqrt{1 - 4}}{3} = \frac{- 1 \pm \mathi \sqrt{3}}{2} \]
We get 2 complex solutions to the ODE: $\mathe^{\left( - \frac{1}{2} +
\frac{\sqrt{3}}{2} \mathi \right) x}, \mathe^{\left( - \frac{1}{2} -
\frac{\sqrt{3}}{2} \mathi \right) x}$. Note that
\[ \mathe^{- \frac{1}{2} x} \cdot \mathe^{\frac{\sqrt{3}}{2} \mathi x} =
   \mathe^{- \frac{1}{2} x}  \left( \cos \frac{\sqrt{3}}{2} x + \mathi \sin
   \frac{\sqrt{3}}{2} x \right) \]
So we get
\[ \left\{\begin{array}{l}
     u (x) = \mathe^{- \frac{1}{2} x} \cos \frac{\sqrt{3}}{2} x\\
     v (x) = \mathe^{- \frac{1}{2} x} \sin \frac{\sqrt{3}}{2} x
   \end{array}\right. \]
are 2 real, linearly independent solutions.

\begin{tmornamented}
  Conclude: if $z$ is a non-real solution to $a_n \lambda^n + \cdots + a_1
  \lambda + a_0 = 0$ then so is $\bar{z}$, and if we write
  \[ z = \alpha + \mathi \beta \]
  we get 2 linearly independent real solutions to the ODE:
  \[ \left\{\begin{array}{l}
       \mathe^{\alpha x} \cos \beta\\
       \mathe^{\alpha x} \sin \beta
     \end{array}\right. \]
\end{tmornamented}

{\underline{Final case}}: the characteristic polynomial has non-real roots
which are multiple roots. Then we multiply the solutions we obtained by powers
of $x$.

For example,
\[ y^{(4)} + 2 y'' + y = 0 \]
Characteristic equation is
\[ \lambda^4 + 2 \lambda^2 + 1 = 0 \]
\[ (\lambda^2 + 1)^2 = 0 \]
roots are $\pm \mathi$ both of multiplicity 2. Get 4 linearly independent real
solutions: {$\cos x, \sin x, x \cos x, x \sin x$}.
\[ y = a \cos x + b \sin x + cx \cos x + dx \sin x \]
Let's show these 4 are linearly independent. Assume linear combinations = 0.
Set $x = 0$ and get $a = 0$. Set $x = \mathpi$ and get $c = 0$. Set $x =
\frac{\mathpi}{2}$ and get $b + \frac{\mathpi}{2} d = 0$. Set $x = -
\frac{\mathpi}{2}$ and get $- b + \frac{\mathpi}{2} d = 0$. Therefore $b, d =
0$.

\begin{tmornamented}
  {\underline{Summarize}}
  
  Suppose the roots of the characteristic polynomial are:
  
  {\underline{real:}} $\lambda_1, \ldots, \lambda_k$ of respective
  multiplicities $r_1, \ldots, r_k$.
  
  {\underline{non-real:}}
  \[ \left\{\begin{array}{l}
       \mu_1, \bar{\mu}_1  \text{ of multiplicity } s_1\\
       \mu_2, \bar{\mu}_2  \text{ of multiplicity } s_2\\
       \vdots\\
       \mu_{\ell}, \bar{\mu}_{\ell}  \text{ of multiplicity } s_{\ell}
     \end{array}\right. \]
  Then we can obtain $r_1 + \cdots + r_k$ linearly independent real solutions
  from $\lambda_1, \ldots, \lambda_k$ and $2 (s_1 + \cdots + s_{\ell})$
  non-real solutions from $\mu_1, \bar{\mu}_1, \ldots, \mu_{\ell},
  \bar{\mu}_{\ell}$.
\end{tmornamented}

{\underline{Example}}

Suppose out ODE has a characteristic polynomial which factors as follows:
\[ \underbrace{(\lambda - 1)^3}_1  \underbrace{(\lambda - 2)^4}_2
   \underbrace{(\lambda^2 + \lambda + 4)^3}_{\frac{1 \pm \mathi \sqrt{15}}{2}}
\]
The degree of the polynomial is 13. We want 13 linearly independent solutions
to the ODE.
\begin{itemize}
  \item $\lambda = 1$: $\mathe^x, x \mathe^x, x^2 \mathe^x$.
  
  \item $\lambda = 2$: $\mathe^{2 x}, x \mathe^{2 x}, x^2 \mathe^{2 x}, x^3
  \mathe^{2 x}$
  
  \item $\lambda = \frac{- 1 + \mathi \sqrt{15}}{2}$: $\mathe^{- \frac{1}{2}
  x} \cos \left( \frac{\sqrt{15} x}{2} \right)$, $\mathe^{- \frac{1}{2} x}
  \sin \left( \frac{\sqrt{15} x}{2} \right), \mathe^{- \frac{1}{2} x} \cos
  \left( \frac{\sqrt{15} x}{2} \right), x \mathe^{- \frac{1}{2} x} \sin \left(
  \frac{\sqrt{15} x}{2} \right), x^2 \mathe^{- \frac{1}{2} x} \cos \left(
  \frac{\sqrt{15} x}{2} \right), x^2 \mathe^{- \frac{1}{2} x} \sin \left(
  \frac{\sqrt{15} x}{2} \right)$.
\end{itemize}
{\underline{Example}}
\[ y^{(3)} - y'' + 2 y' - 8 y = 0 \]
Characteristic equation is
\[ \lambda^3 - \lambda^2 + 2 \lambda - 8 = 0 \]
Look for an integer root. $\lambda = 2$ is a root. Factor out $\lambda - 2$.
\[ \lambda^3 - \lambda^2 + 2 \lambda - 8 = (\lambda - 2) (\lambda^2 + \lambda
   + 4) \]

\subsection{Non-homogeneous linear ODEs}

Given a second order ODE of form:
\[ y'' + p (x) y' + q (x) y = g (x), \quad g (x) \nequiv 0 \]
Note that sums and multiples of solutions will \tmtextbf{not} solve our
equation!

So the set of all solutions is \tmtextbf{not} a vector space.

{\underline{Claim}}: If $y_1, y_2$ both solve the ODE then $y_1 - y_2$ solves
the associated homogeneous equation.

{\underline{Proof}}: We know that:
\begin{eqnarray*}
  y_1'' + p (x) y'_1 + q (x) y_1 & = & g (x)\\
  y_2'' + p (x) y'_2 + q (x) y_2 & = & g (x)
\end{eqnarray*}
Subtract to get:
\[ (y_1'' - y_2'') + p (x)  (y_1' - y_2') + q (x)  (y_1 - y_2) = 0 \]
So $y_1 - y_2$ solves
\[ y'' + p (x) y' + q (x) y = 0 \]
{\underline{Claim:}} Let $y_1$ be a solution of the ODE. Then \tmtextit{every}
solution will be of the form:
\[ y = y_1 + y_0, \]
where $y_0$ solves the associated homogeneous equation. Moreover, every
solution of this form solves the non-homogeneous equation. Therefore, if we
find a $y_1$, we can find the general solution by finding the set of solutions
to the homogeneous equation and add them up.

{\underline{Proof}}: Suppose $y_0$ solves the homogeneous equation. We
substitute $y_1 + y_0$ in the ODE:
\[ (y_1 + y_0)'' + p (x)  (y_1 + y_0)' + q (x)  (y_1 + y_0) \]
\[ \underbrace{(y_1'' + p (x) y'_1 + q (x) y_1)}_{g (x)} + \underbrace{(y_0''
   + p (x) y'_0 + q (x) y_0)}_0 = g (x) + 0 = g (x) \]
Now take any solution $y_2$ of the ODE. We shall show it is of the form $y_0 +
y_1$.

Write:
\[ y_2 = y_1 + (y_2 - y_1) \]
$(y_2 - y_1)$ is the difference of 2 solutions to the non-homogeneous ODE and
so solves the homogeneous equations. So $y_2$ is of the required form.

{\underline{Conclusion}}

To find the general solution to a non-homogeneous linear ODE we need to find
one particular solution and a basis for solution space of the homogeneous
equation.

{\underline{Question}}: How do we find a particular solution to the
non-homogeneous ODE?

\subsubsection{Method of undetermined coefficients}

Applies for the special case where our ODE has constant coefficients. The RHS
can be a non-constant function, though.
\[ a_2 y'' + a_1 y' + a_0 y = g (x), \quad g (x) \nequiv 0 \]
This works if $g (x)$ belongs to one of several families of ``nice''
functions: combination of exponentials, trigonometric functions (sine and
cosine), and polynomials.

Note: multiplication of these ``nice'' functions also counts.

{\underline{Idea}}: We guess a solution which has ``same'' form as $g (x)$. If
$g (x)$ is a poly, then we guess a poly. If $g (x)$ is an exponential, guess
an exponential, etc.

Note: This method does \tmtextbf{not} work for rational functions on RHS!
(Poly divided by poly.)

{\underline{Examples: Which function to guess?}}
\begin{enumerate}
  \item $y'' - 3 y' - 4 y = 3 \mathe^{2 x}$. Guess a solution of form $y (x) =
  A \mathe^{2 x}$. We substitute in the equation and get:
  \[ 4 A \mathe^{2 x} - 6 A \mathe^{2 x} - 4 A \mathe^{2 x} = 3 \mathe^{2 x}
  \]
  Divide by $\mathe^{2 x} \neq 0$.get
  \[ 4 A - 6 A - 4 A = 3 \rightarrow A = - \frac{1}{2} \]
  We found a particular solution $y = - \frac{1}{2} \mathe^{2 x}$.
  
  \
  
  \marginpar{lec 10 05.12.2022}Now find the basis of the associated
  homogeneous equation.
  \[ y'' - 3 y' - 4 y = 0 \]
  \[ \lambda^2 - 3 \lambda - 4 = 0 \rightarrow \lambda = 4, - 1 \]
  So $\mathe^{4 x}, \mathe^{- x}$ is a basis for solutions space of the homo.
  equation. The general solution to the ODE is
  \[ y = - \frac{1}{2} \mathe^{2 x} + c_1 \mathe^{4 x} + c_2 \mathe^{- x},
     \quad c_1, c_2 \in \mathbb{R} \]
  \item $y'' - 3 y' - 4 t = 2 \sin x$.
  
  Wrong guess: $y = A \sin x$. What happens if we substitute?
  \[ - A \sin x - 3 A \cos x - 4 A \sin x = 2 \sin x \]
  \[ - 5 A \sin x - 3 A \cos x = 2 \sin x \quad \forall x \]
  This doesn't hold! Set $x = 0$ and find a contradiction.
  
  Right guess: $y = A \sin x + B \cos x$.
  \[ y' = A \cos x - B \sin x \]
  \[ y'' = - A \sin x - B \cos x \]
  \[ (- A \sin x - B \cos x) - 3 (A \cos x - B \sin x) - 4 (A \sin x + B \cos
     x) = 2 \sin x \]
  \[ (- 5 A + 3 B) \sin x + (- 5 B - 3 A) \cos x = 2 \sin x \]
  Equate coefficients of $\sin x$ and $\cos x$ on both sides.
  \[ \left\{\begin{array}{l}
       \sin x : - 5 A + 3 B = 2\\
       \cos x : - 5 B - 3 A = 0
     \end{array}\right. \Rightarrow A = - \frac{5}{17}, B = \frac{3}{17} \]
  So $y_2 = - \frac{5}{17} \sin x + \frac{3}{17} \cos x$ is a particular
  solution. We've already solved the homogeneous equation (example 1.) The
  general solution is
  \[ - \frac{5}{17} \sin x + \frac{3}{17} \cos x + c_1 \mathe^{4 x} + c_2
     \mathe^{- x}, \quad c_1, c_2 \in \mathbb{R} \]
  \item $y'' - 3 y' - 4 y = 4 x^2 - 1$. Guess a polynomial of the same degree
  as the RHS.
  
  Guess $y = Ax^2 + Bx + C$.
  \begin{eqnarray*}
    y' & = & 2 Ax + B\\
    y'' & = & 2 A
  \end{eqnarray*}
  Substitute:
  \[ 2 A - 3 (2 Ax + B) - 4 (Ax^2 + Bx + C) = 4 x^2 - 1 \]
  Equate coefficients on both sides.
  \[ \left\{\begin{array}{l}
       x^2 : - 4 A = 4\\
       x : - 6 A - 4 B = 0\\
       \tmop{const} : 2 A - 3 B - 4 C = - 1
     \end{array}\right. \Rightarrow A = - 1, B = \frac{3}{2}, C = -
     \frac{11}{8} \]
  $y = - x^2 + \frac{3}{2} x - \frac{11}{8}$ is a particular solution.
  
  \item $y'' - 3 y' - 4 y = - 8 \mathe^x \cos x$. Guess $y = A \mathe^x \cos x
  + B \mathe^x \sin x$ for a particular solution.
  
  \item $y'' - 3 y' - 4 y = 3 \mathe^{2 x} + 2 \sin x$.
  
  We found $y_1 = - \frac{1}{2} \mathe^{2 x}$ solves $y'' - 3 y' - 4 y = 3
  \mathe^{2 x}$ and $y_2 = - \frac{5}{17} \sin x + \frac{3}{17} \cos x$ solves
  $y'' - 3 y' - 4 y = 2 \sin x$.
  
  So if we substitute $y_1 + y_2$ in LHS we get: $3 \mathe^{2 x} + 2 \sin x$.
  
  This is how you deal with a complex RHS. Don't try to solve it all in one
  shot. Divide the RHS into easily solvable groups and add them up later.
  
  \item $y'' + 9 y = 2 \sin 3 x$.
  
  Here, as before, we try $y = A \sin 3 x + B \cos 3 x$.
  \begin{eqnarray*}
    y' & = & 3 A \cos 3 x - 3 B \sin 3 x\\
    y'' & = & - 9 (A \sin 3 x + B \cos 3 x)
  \end{eqnarray*}
  When we substitute we get
  \[ \underbrace{- 9 (A \sin 3 x + B \cos 3 x) + 9 (A \sin 3 x + B \cos 3
     x)}_{= 0} = 2 \sin 3 x \]
  \[ 0 = 2 \sin 3 x \Rightarrow \text{no solution?!} \]
  What went wrong? If you look at the associated homogeneous equation, it
  turns out that $y = A \sin 3 x + B \cos 3 x$ solves it.
  \[ y'' + 9 y = 0 \]
  \[ \lambda^2 + 9 = 0 \rightarrow \lambda = \pm 3 \mathi \]
  We get a basis for solution space: $\{ \sin 3 x, \cos 3 x \}$.
  
  We have to guess something else. What works here --- Multiply by $x$.
  
  Guess instead $y = x (A \cos 3 x + B \sin 3 x)$
  \begin{eqnarray*}
    y' & = & A \cos 3 x + B \sin 3 x + x (- 3 A \sin 3 x + 3 B \cos 3 x)\\
    y'' & = & - 3 A \sin 3 x + 3 B \cos 3 x - 3 A \sin 3 x + 3 B \cos 3 x\\
    & + & x (- 9 A \cos 3 x - 9 B \sin 3 x)\\
    y'' & = & - 6 A \sin 3 x + 6 B \cos 3 x - 9 x (A \cos 3 x + B \sin 3 x)
  \end{eqnarray*}
  Substitute:
  \[ y'' + 9 y = - 6 A \sin 3 x + 6 B \cos 3 x = 2 \sin 3 x \]
  Equate coefficients on both sides and get $A = - \frac{1}{3}, B = 0$. The
  particular solution is $y = - \frac{1}{3} x \cos 3 x$.
  
  \item $y^{(4)} - y' = x^3 + 2 x^2 + 3$. Find solution to homogeneous
  version. Note that we can't guess a polynomial of degree 3 because the LHS
  has a characteristic polynomial of a higher degree.
  \[ \lambda (\lambda - 1) (\lambda^2 + 2 \lambda + 1) = \lambda (\lambda^3 -
     1) = 0 \]
  Roots are $\lambda = 0, \lambda = 1, \lambda = \frac{- 1 \pm \mathi
  \sqrt{3}}{2}$.
  
  Get 4 linearly independent solutions: $1, \mathe^x, \mathe^{- \frac{1}{2} x}
  \cos \frac{\sqrt{3} x}{2}, \mathe^{- \frac{1}{2} x} \sin \frac{\sqrt{3}
  x}{2}$.
  
  It is clear that polynomials of degree 0 solve the homogeneous equation,
  however it wouldn't benefit to finding a particular solution.
  
  Guess a 4th degree polynomial, which would give us a third degree polynomial
  at both RHS and LHS. $y = x (Ax^3 + Bx^2 + Cx + D \nobracket$.
  \begin{eqnarray*}
    y' & = & 4 Ax^3 + 3 Bx^2 + 2 Cx + D\\
    y'' & = & 12 Ax^2 + 6 Bx + 2 C\\
    y''' & = & 24 Ax + 6 B\\
    y^{(4)} & = & 24 A
  \end{eqnarray*}
  Substitute and get:
  \[ 24 A - (4 Ax^3 + 3 Bx^2 + 2 Cx + D) = x^3 + 2 x^2 + 3 \]
  Equate coefficients of equal powers of $x$ on both sides:
  \[ \left\{\begin{array}{l}
       x^3 : \quad - 4 A = 1 \rightarrow A = - \frac{1}{4}\\
       x^2 : \quad - 3 B = 2 \rightarrow B = - \frac{2}{3}\\
       x : \quad 2 C = 0 \rightarrow C = 0\\
       1 : \quad 24 A - D = 3 \rightarrow D = - 9
     \end{array}\right. \]
  The general solution is: $y = - \frac{1}{4} x^4 - \frac{2}{3} x^2 - 9 x +
  c_1 + c_2 \mathe^x + \mathe^{- \frac{1}{2} x}  \left( c_3 \cos
  \frac{\sqrt{3}}{2} x + c_4 \sin \frac{\sqrt{3}}{2} x \right)$.
  
  {\underline{Note}}: If the ODE were $y^{(4)} - y'' = x^3 + 2 x^2 + 3$ then
  we would have chosen a 5th degree polynomial as a particular solution.
  
  {\underline{Note}}: If the homogeneous equation would have had repeated
  roots and RHS involved a solution to the homogeneous equation, we would have
  multiplied by higher powers of $x$.
  
  \item $y'' + 3 y' = 2 x^4 + x^2 \mathe^{- 3 x} + \sin 3 x$. Solve the
  homogeneous equation.
  \[ \lambda (\lambda + 3) = \lambda^2 + 3 \lambda = 0 \]
  $\lambda = 0, - 3$, giving the basis: $1, \mathe^{- 3 x}$.
  
  It won't be enough to guess either a poly. of degree 4 of an exponent
  multiplied by $x^2$. Divide the problem into parts.
  \begin{enumerateromancap}
    \item Find a particular solution when RHS is $2 x^4$. Guess a 5th degree
    poly. with constant zero (as it's redundant).
    \[ y_1 = x (A_4 x^4 + A_3 x^3 + A_2 x^2 + A_1 x + A_0) \]
    \item Now find a particular solution when RHS is $x^2 \mathe^{- 3 x}$.
    
    If we guess (quadratic)$\mathe^{- 3 x}$, i.e. $y_2 = (B_2 x^2 + B_1 x)
    \mathe^{- 3 x}$, then
    \begin{eqnarray*}
      y_2' & = & (2 B_2 x + B_1) \mathe^{- 3 x} - 3 \mathe^{- 3 x} (B_2 x^2 +
      B_1 x)\\
      y''_2 & = & 2 B_2 \mathe^{- 3 x} + B_1 \mathe^{- 3 x} - 3 \mathe^{- 3 x}
      (2 B_2 x + B_1)\\
      & + & - 3 \mathe^{- 3 x} (2 B_2 x + B_1) + 9 \mathe^{- 3 x} (B_2 x^2 +
      B_1 x)
    \end{eqnarray*}
    Substitute on LHS:
    \[ (9 \mathe^{- 3 x} B_2 x^2 + \cdots) + 3 (- 3 \mathe^{- 3 x} B_2 x^2 +
       \cdots +) = \left( \text{lin. poly.} \right) \mathe^{- 3 x} \]
    We don't have a high enough order poly. multiplied by $\mathe^{- 3 x}$.
    Guess instead {$y_2 = x ((B_2 x^2 + B_1 x + B_0) \mathe^{- 3 x})$}.
    
    \item Now find particular solution when RHS is $\sin 3 x$. Guess ${y_3 =
    C_1 \sin 3 x + D_2 \cos 3 x} {}$.
  \end{enumerateromancap}
  The general solution will be $y_1 + y_2 + y_3 + c_1 + c_2 \mathe^{- 3 x}$.
  
  
  
  \ 
\end{enumerate}
\begin{tmornamented}
  {\underline{Conclude}}
  \begin{itemize}
    \item First, solve the associated homogeneous equation
    
    \item If the RHS is a solution to the homogeneous equation, guess a
    particular solution of the form:
    \[ (\tmop{power} \tmop{of} x) \cdot (\tmop{RHS} \tmop{form}) \]
    Such that the form of LHS matches the form of RHS.
  \end{itemize}
  \tmsession{matlab}{default}{}
\end{tmornamented}

{\underline{Cases where the RHS is split (not differentiable)}}

For example,
\[ u'' + u = f (t), \quad \left\{\begin{array}{l}
     u (0) = 0\\
     u' (0) = 0
   \end{array}\right. \]
\[ f (t) = \left\{\begin{array}{l}
     5 t, \quad 0 \leq t \leq \mathpi\\
     5 (2 \mathpi - t), \quad \mathpi \leq t \leq 2 \mathpi\\
     0, \quad t > 2 \mathpi
   \end{array}\right. \]
$f (t)$ looks like this:

\begin{center}
  \raisebox{-0.139275014730413\height}{\includegraphics[width=7.84581201626656cm,height=3.95182342909616cm]{differential-equations-for-chemists-17.pdf}}
\end{center}

Solve associated homogeneous equation:
\[ x^2 + 1 = 0 \Longleftrightarrow x = \pm \mathi \]
General solution is a linear combination of $\sin t, \cos t$.

Particular solution on $[0, \mathpi]$: In this interval $f (t)$ is a
polynomial. Guess $u = At + B$, substitute and get
\[ At + B = 5 t \rightarrow A = 5, B = 0 \]
General solution on $[0, \mathpi] : x (t) = a_1 \cos t + a_2 \sin t + 5 t$.
Inserting $t = 0$ gives $a_1, a_2$.

Particular solution on $[\mathpi, 2 \mathpi] :$ Guess again $u = At + B$,
substitute and get
\[ At + B = 10 \mathpi - 5 t \rightarrow A = - 5, B = 10 \mathpi \]
General solution on $[\mathpi, 2 \mathpi] : y (t) = b_1 \cos t + b_2 \sin t -
5 t + 10 \mathpi$.

General solution on $[2 \mathpi, \infty) : z (t) = c_1 \cos t + c_2 \sin t$.

We want a unique solution satisfying the ICs, which is twice differentiable
(equivalent to order of the ODE). We need to determine the coefficients $a_{1,
2}, b_{1, 2}, c_{1, 2}$ so that this condition holds.

Setting $t = 0 :$
\[ x (0) = a_1 = 0 \]
\[ x' (0) = a_2 + 5 = 0 \rightarrow a_2 = - 5 \]
\[ x (t) = - 5 \sin t + 5 t \]
Now we need to make sure that $x (\mathpi) = y (\mathpi), x' (\mathpi) = y'
(\mathpi)$.
\[ x (\mathpi) = 5 \mathpi, x' (\mathpi) = 10 \]
Set ``initial conditions'' for $y (t)$: $y (\mathpi) = 5 \mathpi, y' (\mathpi)
= 10$.
\[ y (\mathpi) = b_1 \cos \mathpi + b_2 \sin \mathpi - 5 \mathpi + 10 \mathpi
   = - b_1 + 5 \mathpi \rightarrow b_1 = 0 \]
\[ y' (\mathpi) = - b_2 - 5 \rightarrow b_2 = - 15 \]
\[ y (t) = - 15 \sin t - 5 t + 10 \mathpi \]
Make sure $y (2 \mathpi) = z (2 \mathpi), y' (2 \mathpi) = z' (2 \mathpi)$. $y
(2 \mathpi) = 0, y' (2 \mathpi) = - 20$.
\[ z (2 \mathpi) = c_1 \cos 2 \mathpi + c_2 \sin 2 \mathpi \rightarrow c_1 = 0
\]
\[ z' (2 \mathpi) = - c_1 \sin 2 \mathpi + c_2 \cos 2 \mathpi \rightarrow c_2
   = - 20 \]
\[ z (t) = - 20 \sin t \]
In summary, we have a unique solution:
\[ u (t) = \left\{\begin{array}{l}
     - 5 \sin t + 5 t, \quad 0 \leq t \leq \mathpi\\
     - 15 \sin t - 5 t + 10 \mathpi, \quad \mathpi < t \leq 2 \mathpi\\
     - 20 \sin t, \quad t > 2 \mathpi
   \end{array}\right. \]
We constructed $u (t)$ so that it is continuous everywhere. What about the
second derivative?
\[ u'' (t) = \left\{\begin{array}{l}
     5 \sin t, \quad 0 \leq t \leq \mathpi\\
     15 \sin t, \quad \mathpi < t \leq 2 \mathpi\\
     20 \sin t, \quad t > 2 \mathpi
   \end{array}\right. \]
The second derivative is continuous (which is what we expected). Note though
that at $\mathpi$ and $2 \mathpi$, $u''' (t)$ does not exist!

Our method of indeterminate coefficients works also for a split RHS case, so
long that the solution is differentiable and continuous within the given
intervals.

\subsubsection{Mechanical and electrical vibrations}

Start with the mechanical case: mass on a spring.

\begin{center}
  \raisebox{-0.00294951263912252\height}{\includegraphics[width=8.1541551882461cm,height=2.84645152826971cm]{differential-equations-for-chemists-18.pdf}}
\end{center}

When we hang a mass on a spring it lengthens by $L$.

When in a state of equilibrium, by Newton's 2nd law: $\sum F = 0$.
\begin{eqnarray*}
  \text{Force downward} & = & mg\\
  \text{Force upward exerted by spring} & = & F_s = kL
\end{eqnarray*}
So $mg = F_s = kL$, where $k$ is the spring's constant. (The stronger the
spring, the smaller $L$ would be.)

We act on the spring with a new force $F (t)$ and it displaces the mass by $u
(t)$ at time $t$.

\begin{center}
  \raisebox{-0.175191283376997\height}{\includegraphics[width=4.53815754952119cm,height=3.78046700773974cm]{differential-equations-for-chemists-19.pdf}}
\end{center}

{\underline{Forces acting on mass}}:
\begin{itemize}
  \item $F (t)$, external force (down)
  
  \item $mg$, gravity (down).
  
  \item Force exerted by the spring (up)
  
  \item Damping force (drag) (up), which is proportionate to the velocity.
  $\gamma u' (t)$.
\end{itemize}
Sum of forces is $ma (t)$.
\[ mu'' (t) = F (t) + mg - k (L + u (t)) - \gamma u' (t) \]
\[ mu'' = F (t) - ku - \gamma u' \]
\marginpar{lec 11 11.12.22}Rearrange:
\[ mu'' + \gamma u' + ku = F (t) \]
This is a 2nd order linear ODE with constant coefficients. Initial conditions:
${u (0) = u_0}$, $u' (0) = v_0$.

{\underline{Example}}: A mass weighing 4N stretches a spring by 10 cm. We
stretch another 20 cm down and let go. The mass is in a sticky medium that
exerts a resistance of 6N when mass has velocity of {\SI{1}{m}{$s^{- 1}$}}. At
$t = 0$ $F = 0$.
\[ mu'' + \gamma u' + ku = 0 \]
\[ mg = 4 \rightarrow m \approx \SI{0.4}{\tmop{kg}} \]
\[ mg = kL \rightarrow k = \frac{\SI{0.4}{N}}{\SI{0.1}{m}} =
   \SI{40}{\tmop{kg}}{s^{- 2}} \]
\[ \gamma u' = 6 \text{ when } u' = \SI{1}{m}{s^{- 1}} \rightarrow \gamma =
   \SI{6}{\tmop{kg}}{s^{- 1}} \]
Initial conditions are $u (0) = \SI{0.2}{m}$ and $u' (0) = 0$. Get:
\[ \frac{4}{10} u'' + 6 u' + 40 u = 0 \]
\[ u'' + 15 u' + 100 u = 0 \]
Solve characteristic equation:
\[ \lambda^2 + 15 \lambda + 100 = 0 \]
\[ \lambda_{1, 2} = \frac{- 15 \pm \sqrt{} 225 - 400}{2} = \frac{- 15 \pm 5
   \sqrt{7} \mathi}{2} \]
We actually expect non-real roots, because that means the mass oscillates!

General solution:
\[ u (t) = c_1 \mathe^{- \frac{15}{2} t} \cos \frac{5 \sqrt{7}}{2} t + c_2
   \mathe^{- \frac{15}{2} t} \sin \frac{5 \sqrt{7}}{2} t \]
Set ICs:
\begin{eqnarray*}
  u (0) & = & c_1 = \frac{1}{5}
\end{eqnarray*}
\[ u' (t) = - \frac{15}{2} \left( c_1 \cos \frac{5 \sqrt{7}}{2} t + c_2 \sin
   \frac{5 \sqrt{7}}{2} t \right) + \mathe^{- \frac{15}{2} t} \left( - c_1 
   \frac{5 \sqrt{7}}{2} \sin \frac{5 \sqrt{7}}{2} t + c_2  \frac{5
   \sqrt{7}}{2} \cos \frac{5 \sqrt{7}}{2} t \right) \]
\[ u' (0) = - \frac{15}{2} c_1 + \frac{5 \sqrt{7}}{2} c_2 = 0 \rightarrow c_2
   = \frac{3}{5 \sqrt{7}} \]
The unique solution is: $u (t) = \mathe^{- \frac{15}{2} t} \left( \frac{1}{5}
\cos \frac{5 \sqrt{7}}{2} t + \frac{3}{5 \sqrt{7}} \sin \frac{5 \sqrt{7}}{2} t
\right)$

{\underline{Another example}}: 2-degree of freedom spring-mass system.

\begin{center}
  \raisebox{-0.267300825021549\height}{\includegraphics[width=4.27602321920504cm,height=4.26131444313263cm]{differential-equations-for-chemists-20.pdf}}
\end{center}

$u_i (t)$ is the position of mass $i$ at time $t$.

We assume there is no damping and no external force: $F (t) = 0, \gamma = 0$.
\begin{eqnarray*}
  m_1 u''_1 & = & - k_1 u_1 + (u_2 - u_1) k_2\\
  m_2 u''_2 & = & - k_2 (u_2 - u_1)
\end{eqnarray*}
Solve by reducing to one 4th order ODE. Say $m_1 = m_2 = 1, k_1 = 3, k_2 = 2$.
\begin{eqnarray*}
  u_1'' & = & - 3 u_1 + (u_2 - u_1) \cdot 2\\
  u_2'' & = & - 2 (u_2 - u_1)
\end{eqnarray*}
\begin{eqnarray*}
  u_1'' & = & - 5 u_1 + 2 u_2 \rightarrow u_2 = \frac{u_1'' + 5 u_1}{2}\\
  u_2'' & = & 2 u_1 - 2 u_2
\end{eqnarray*}
Substitute $u_2$ and get
\[ \left( \frac{u_1'' + 5 u_1}{2} \right)'' = 2 u_1 - (u_1'' + 5 u_1) \]
\[ u_1^{(4)} + 7 u_1'' + 6 u_1 = 0 \]
Solve characteristic equation: $\lambda^4 + 7 \lambda^2 + 6 = 0$
\[ \lambda = \pm \mathi, \pm \mathi \sqrt{6} \]
General solution:
\[ u_1 = c_1 \cos t + c_2 \sin t + c_3 \cos \sqrt{6} t + c_4 \sin \sqrt{6} t
\]
Taking IC:
\[ \left\{\begin{array}{l}
     u_1 (0) = 1\\
     u_1' (0) = 0\\
     u_2 (0) = 2\\
     u_2' (0) = 0
   \end{array}\right. \]
yields:
\[ u_1 = \cos t \]
\[ u_2 = \frac{u_1'' + 5 u_1}{2} = \frac{- \cos t + 5 \cos 5}{2} = 2 \cos t \]
\marginpar{lec 12 12.12.22}Special cases of $mu'' + \gamma u' + ku = F (t)$:

{\underline{Undamped free vibrations}}

Here $F (t) = 0$ and $\gamma = 0$. Equation is now $mu'' + ku = 0$.
\[ u'' + \frac{k}{m} u = 0 \]
We need to solve $\lambda^2 + \frac{k}{m} = 0$. $\lambda_{1, 2} = \pm
\sqrt{\frac{k}{m}}$. Denote $\sqrt{\frac{k}{m}} = \omega_0$. General solution
is
\[ u (t) = A \cos \omega_0 t + B \sin \omega_0 t \]
Rewrite using polar coordinates:

\begin{center}
  \raisebox{-0.183352748654796\height}{\includegraphics[width=2.99549062049062cm,height=2.9560212514758cm]{differential-equations-for-chemists-21.pdf}}
\end{center}
\[ R = \sqrt{A^2 + B^2}, \delta = \arctan \frac{B}{A} \]
\[ A = R \cos \delta, B = R \sin \delta \]
So
\begin{eqnarray*}
  u (t) & = & R (\cos \delta \cos \omega_0 t + \sin \delta \sin \omega_0 t)\\
  & = & R \cos (\omega_0 t - \delta)
\end{eqnarray*}
We get oscillations with period $T = \frac{2 \mathpi}{\omega_0} = 2 \mathpi
\sqrt{\frac{m}{k}}$.

$\omega_0$ is called the ``natural frequency'' of the vibration, $R$ is the
``amplitude'', and $\delta$ is the ``phase angle''.

{\underline{Damped free vibrations}}

$F (t) = 0$, so we have
\[ mu'' + \gamma y' + ku = 0 \]
Roots of the char. poly. are
\[ r_1, r_2 = \frac{- \gamma \pm \sqrt{\gamma^2 - 4 mk}}{2 m} =
   \frac{\gamma}{2 m} \left[ - 1 \pm \sqrt{1 - \frac{4 mk}{\gamma^2}} \right]
\]
You can see that the square roots may or may not be real, but their real part
is \tmtextbf{always} negative.

3 cases:
\begin{enumerate}
  \item $\gamma^2 - 4 mk > 0$.
  
  $r_1, r_2$ are real. Since $\gamma > \sqrt{\gamma^2 - 4 mk}$, roots are both
  negative.
  
  The general solution is
  \[ u (t) = A \mathe^{r_1 t} + B \mathe^{r_2 t} \]
  As $t \rightarrow \infty$, $u (t) \rightarrow 0$, no vibrations! This
  results in \tmtextit{over-damped motion}.
  
  \item $\gamma^2 - 4 mk = 0$.
  \[ u (t) = \mathe^{- \frac{\gamma}{2 m} t}  (A + Bt) \]
  Still no vibrations! This is called \tmtextit{critical damping}.
  
  \item $\gamma^2 - 4 m \nobracket k \nobracket < 0$.
  \[ r_{1, 2} = \frac{- \gamma}{2 m} \pm \mathi \frac{\sqrt{4 km -
     \gamma^2}}{2 m} \equiv - \frac{\gamma}{2 m} \pm \mathi \mu \]
  General solution:
  \[ u (t) = \mathe^{- \frac{\gamma}{2 m} t}  (A \cos (\mu t) + B \sin (\mu
     t)) \]
  Here we have vibrations but as $t \rightarrow \infty$ they die away ($u (t)
  \rightarrow 0$).
  
  As before we can rewrite this is
  \[ u (t) = R \mathe^{- \frac{\gamma}{2 m} t} \cos (\mu t - \delta) \]
  This is called \tmtextit{damped vibration}, or \tmtextit{small damping}.
\end{enumerate}
{\underline{Electrical vibrations}}

\begin{center}
  \raisebox{-0.366933888054844\height}{\includegraphics[width=6.59215531942805cm,height=5.01342975206612cm]{differential-equations-for-chemists-22.pdf}}
\end{center}

$Q (t)$ is the charge within the circuit and $I (t)$ is the current. $I (t) =
\frac{\mathd Q}{\mathd t}$.

The voltage drops on each unit:
\begin{itemize}
  \item On inductor: $L \cdot \frac{\mathd I}{\mathd t}$, \quad$L > 0$
  
  \item On resistor: $R \cdot I$, \quad$R > 0$
  
  \item On capacitor: $\frac{Q}{C}$, \quad$C > 0$
\end{itemize}
Kirchhoff's's law says that the sum of voltage that drops on circuit is equal
to the applied voltage $E (t)$.
\[ L \frac{\mathd I}{\mathd t} + RI + \frac{1}{C} Q = E (t) \]
Substitute $Q = \frac{\mathd I}{\mathd t}$
\[ LQ'' + RQ' + \frac{1}{C} Q = E (t) \]
You may also differentiate the first equation and get
\[ LI'' + RI' + \frac{1}{C} I = E' (t) \]
Get the ``mechanical-electrical analogy''
\[ \left\{\begin{array}{l}
     \gamma \leftrightarrow R\\
     m \leftrightarrow L\\
     k \leftrightarrow \frac{1}{C}
   \end{array}\right. \]
Note that ICs for $Q$ can yield ICs for $I$, as if we're given
\[ \left\{\begin{array}{l}
     Q (t_0) = Q_0\\
     Q' (t_0) = I_0
   \end{array}\right. \]
Then we can derive $I' (t_0)$ to form the equation:
\[ LI' + RI + \frac{1}{C} Q = E (t) \]
\[ I' (t_0) = \frac{E (t_0) - \frac{1}{C} Q_0 - RI_0}{L} \]
Let's treat the mechanical version again, but with force applied:

\

\

{\underline{Forced mechanical vibrations}}
\[ mu'' + \gamma u' + ku = F (t) \]
$F (t) \neq 0$ and is periodic. For example, $F (t) = F_0 \cos \omega t$.
\begin{enumerate}
  \item No damping. $\gamma = 0$
  \[ mu'' + ku = F_0 \cos \omega t \]
  Recall that solution to the homogeneous equation was $A \sin \omega_0 t + B
  \cos \omega_0 t$, $\omega_0 = \sqrt{\frac{k}{m}} .$
  \begin{enumerateromancap}
    \item $\omega_0 \neq \omega$. Guess particular solution of form
    \begin{eqnarray*}
      u & = & c_1 \cos \omega t + c_2 \sin \omega t\\
      u'' & = & - c_1 \omega^2 \cos \omega t - c_2 \omega^2 \sin \omega t
    \end{eqnarray*}
    \[ - m \omega^2  (c_1 \cos \omega t + c_2 \sin \omega t) + k (c_1 \cos
       \omega t + c_2 \sin \omega t) = F_0 \cos \omega t \]
    Equate coefficient on both sides.
    \begin{eqnarray*}
      0 & = & - mc_2 \omega^2 + kc_2 = c_2  (k - m \omega^2)\\
      F_0 & = & - mc_1 \omega^2 + kc_1
    \end{eqnarray*}
    We assumed $\omega_0 \neq \omega$, so $k \neq m \omega^2$ and we get $c_2
    = 0$.
    \[ c_1 = \frac{F_0}{k - m \omega^2} \]
    General solution in this case:
    \[ u (t) = \frac{F_0}{k - m \omega^2} \cos \omega t + A \sin \omega_0 t +
       B \cos \omega_0 t \]
    The behavior of solution depends on the relation between $\omega_0$ and
    $\omega$. $k - m \omega^2 = m (\omega_0 - \omega^2)$. If very close,
    coefficient of $\cos \omega t$ is very large. In other words, the
    oscillation amplitude will increase as the force oscillation frequency
    approaches the natural frequency of the spring.
    
    Taking ICs, $u (0) = 0, u' (0) = 0$, we get
    \[ \left\{\begin{array}{l}
         B = - \frac{F_0}{m (\omega_0^2 - \omega^2)}\\
         A = 0
       \end{array}\right. \]
    Our solution is then
    \[ u (t) = \frac{F_0}{m (\omega_0^2 - \omega^2)}  (\cos \omega t - \cos
       \omega_0 t) \]
    Using trig. identities we can rewrite solution as a product of sine
    functions:
    \[ u (t) = \frac{2 F_0}{m (\omega_0^2 - \omega^2)} \sin \left(
       \frac{\omega_0 - \omega}{2} t \right) \sin \left( \frac{\omega_0 +
       \omega}{2} t \right) \]
    We get ``beats''.
    
    \begin{center}
      \raisebox{-0.0010371510756391\height}{\includegraphics[width=11.9161255411255cm,height=8.09491014036469cm]{differential-equations-for-chemists-23.pdf}}
    \end{center}
    
    Note: the $\sin \left( \frac{\omega_0 - \omega}{2} t \right)$ term is
    responsible to the outer oscillations and the $\sin \left( \frac{\omega_0
    + \omega}{2} t \right)$ term is responsible to the inner oscillations.
    
    \item $\omega  = \omega_0$. External force is applied at resonance.
    \[ mu'' + ku = F_0 \cos \omega_0 t \]
    Guess a particular solution: $c_1 t \cos \omega_0 t + c_2 t \sin \omega_0
    t$.
    
    General solution:
    \[ u (t) = A \cos \omega_0 t + B \sin \omega_0 t + \frac{F_0}{2 m
       \omega_0} t \sin \omega_0 t \]
    The solution diverges!
    
    \raisebox{-0.0010399021842008\height}{\includegraphics[width=11.9161255411255cm,height=8.07349468713105cm]{differential-equations-for-chemists-24.pdf}}
  \end{enumerateromancap}
\end{enumerate}

\section{System of ODES}

\subsection{Examples of problems represented by a system of ODEs}

\begin{enumerate}
  \item 2nd order system in $u_1, u_2$.
  
  \begin{center}
    \raisebox{-0.267300825021549\height}{\includegraphics[width=4.27602321920504cm,height=4.26131444313263cm]{differential-equations-for-chemists-25.pdf}}
  \end{center}
  
  \item 2-degree of freedom spring-mass system with dash-pot.
  
  \raisebox{-0.5\height}{\includegraphics[width=14.8909550045914cm,height=8.92450478814115cm]{differential-equations-for-chemists-26.pdf}}
  
  System of motion equations (Assume no damping):
  \[ \left\{\begin{array}{l}
       m_1 x_1'' = F_1 (t) - k_1 x_1 + k_2  (x_2 - x_1)\\
       m_2 x_2'' = F_2 (t) - k_2  (x_2 - x_1) - k_3 x_2
     \end{array}\right. \]
  or:
  \begin{eqnarray*}
    m_1 x_1'' & = & - (k_1 + k_2) x_1 + k_2 x_2 + F_1 (t)\\
    m_2 x_2'' & = & k_2 x_1 - (k_2 + k_3) x_2 + F_2 (t)
  \end{eqnarray*}
  This is a linear 2nd order system of ODEs. In matrix form:
  \[ \left(\begin{array}{c}
       x_1''\\
       x_2''
     \end{array}\right) = \left(\begin{array}{cc}
       - \frac{(k_1 + k_2)}{m_1} & \frac{k_2}{m_1}\\
       \frac{k_2}{m_2} & - \frac{k_2 + k_3}{m_2}
     \end{array}\right) \left(\begin{array}{c}
       x_1\\
       x_2
     \end{array}\right) + \left(\begin{array}{c}
       \frac{F_1 (t)}{m_1}\\
       \frac{F_2 (t)}{m_2}
     \end{array}\right) \]
  \item Parallel electrical circuit.
  
  \begin{center}
    \raisebox{-0.00227920227920228\height}{\includegraphics[width=3.9806178669815cm,height=3.68358913813459cm]{differential-equations-for-chemists-27.pdf}}
  \end{center}
  \[ \left\{\begin{array}{l}
       V (t) = \text{ voltage at time $t$}\\
       I (t) = \text{ current at time } t
     \end{array}\right. \]
  Get:
  \[ \left\{\begin{array}{l}
       \frac{\mathd I (t)}{\mathd t} = \frac{V (t)}{L}\\
       \frac{\mathd V (t)}{\mathd t} = - \frac{I (t)}{C} - \frac{V (t)}{RC}
     \end{array}\right. \]
  This is a linear first order system.
  \[ \left[\begin{array}{c}
       I (t)\\
       V (t)
     \end{array}\right]' = \left[\begin{array}{cc}
       0 & \frac{1}{L}\\
       - \frac{1}{C} & - \frac{1}{\tmop{RC}}
     \end{array}\right]  \left[\begin{array}{c}
       I (t)\\
       V (t)
     \end{array}\right] \]
  \item Predator-prey situation, model of Lotka \& Volterra (1925).
  \begin{itemize}
    \item population of food fish = $x (t)$
    
    \item population of sharks = $y (t)$
  \end{itemize}
  Principles:
  \begin{enumerate}
    \item If no predators, prey pop. increases at a constant rate $a > 0$.
    \[ \frac{\mathd x}{\mathd t} = ax \]
    \item If no prey, predator pop. decreases at constant rate $c > 0$.
    \[ \frac{\mathd y}{\mathd t} = - cy \]
    \item If both prey and predators are present, number of encounters is
    proportionate to $xy$.
  \end{enumerate}
  Result is net decline of $bxy$ in fish and increase of $rxy$ in sharks. ($r,
  b > 0$)
  \[ \left\{\begin{array}{l}
       \frac{\mathd x}{\mathd t} = ax - bxy = x (a - by)\\
       \frac{\mathd y}{\mathd t} = - cx + rxy = y (- c + rx)
     \end{array}\right. \]
  This is a first order non-linear system.
  
  Inspect equilibrium of populations: $x = 0 = y$ \tmtextbf{or} $y =
  \frac{a}{b}$ \& $x = \frac{c}{r}$.
  
  {\underline{Note:}} The system:
  \[ \left\{\begin{array}{l}
       y_1' = y_1\\
       y_2' = y_2
     \end{array}\right. \]
  Clearly cannot express $y_2$ in terms of $y_1$ and reduce to one equation.
\end{enumerate}
{\underline{In general:}}

A first order system of ODEs is of the form:
\begin{eqnarray*}
  y_1' (x) & = & f_1 (x, y, \ldots, y_n)\\
  y_2' (x) & = & f_2 (x, y, \ldots, y_n)\\
  \vdots &  & \\
  y'_n (x) & = & f_n (x, y, \ldots, y_n)
\end{eqnarray*}
We call it \tmtextit{linear} if $f_0, \ldots, f_n$ are linear in $y_1, \ldots,
y_n$.

So $f_i (x, y_1, \ldots, y_n) = p_{i 1} (x) y_1 + p_{i_2} (x) y_2 + \cdots +
p_{i_n} (x) y_n + g_i (x)$ where $p_{i_1}, \ldots, p_{i_n}$ are functions of
$x$.

Linear systems can be written in matrix form:
\[ \left[\begin{array}{c}
     y_1' (x)\\
     \vdots\\
     y'_n (x)
   \end{array}\right] = \left[\begin{array}{ccc}
     p_{1_1} (x) & \ldots & p_{1_n} (x)\\
     \vdots & \ddots & \\
     p_{n_1} (x) &  & p_{n_n} (x)
   \end{array}\right] \left[\begin{array}{c}
     y_1 (x)\\
     \vdots\\
     y_n (x)
   \end{array}\right] + \left[\begin{array}{c}
     g_1 (x)\\
     \vdots\\
     g_n (x)
   \end{array}\right] \]
Or
\[ \vec{Y}' = A \vec{Y} + \vec{b} \]
{\underline{Example}}
\[ \left\{\begin{array}{l}
     y_1' = x^2 y_1 + \mathe^x y_2 + \sin xy _3 + x^2\\
     y_2' = xy_2 + y_3 + \mathe^{2 x}\\
     y_3' = \sqrt{x} y_1 + 2 y_3 + \ln x
   \end{array}\right. \]
\[ \left[\begin{array}{c}
     y_1\\
     y_2\\
     y_3
   \end{array}\right]' = \left[\begin{array}{ccc}
     x^2 & \mathe^x & \sin x\\
     0 & x & 1\\
     \sqrt{x} & 0 & 2
   \end{array}\right] \left[\begin{array}{c}
     y_1\\
     y_2\\
     y_3
   \end{array}\right] + \left[\begin{array}{c}
     x^2\\
     \mathe^{2 x}\\
     \ln x
   \end{array}\right] \]
A solution is a vector of functions $\left[\begin{array}{ccc}
  y_1 & \ldots & y_n
\end{array}\right]^T$ satisfying the system.

ICs: $y_1 (a) = b_1, y_2 (a) = b_2, \ldots, y_n (a) = b_n$.

\subsection{Existence \& Uniqueness theorem for first order systems}

Given
\begin{eqnarray*}
  y_1' (x) & = & f_1 (x, y, \ldots, y_n)\\
  y_2' (x) & = & f_2 (x, y, \ldots, y_n)\\
  \vdots &  & \\
  y'_n (x) & = & f_n (x, y, \ldots, y_n)
\end{eqnarray*}
and ICs
\[ y_1 (a) = b_1, y_2 (a) = b_2, \ldots, y_n (a) = b_n \]
such that $f_1, \ldots, f_n$ are continuous in $x, y_1, \ldots, y_n$ and
$\partial f_i / \partial y_j$ exist and are continuous in some neighborhood of
$(a, b_1, \ldots, b_n)$ in $\mathbb{R}^{n + 1}$, there exists a unique
solution satisfying ICs defined for some interval around $a$. This holds for
linear systems if and only if $g_i, p_{i, j}$ are continuous.

\marginpar{lec 13 18.12.22}

\begin{tmornamented}
  {\underline{Theorem}}: The {\tmem{cardinality}} (number of elements in a
  group) of a basis is determined uniquely. This cardinality is the
  {\tmem{dimension}} of the space.
  
  {\underline{Theorem}}: If the dimension of a space $w$ is $k$, then any
  linearly independent set of $k$ elements will be a basis. Also, any spanning
  set with $k$ elements will be a basis.
  
  Note that:
  \begin{itemize}
    \item The dimension of the vector space of all real functions in infinite.
    ($2^{\aleph_0}$)
    
    \item The dimension of vector space of polynomial functions is $\aleph_0$
    are $1, x, x^2, \ldots$ is a basis.
  \end{itemize}
\end{tmornamented}

\subsection{Reducing a higher order ODE to a system of first order ODEs}

Any higher order linear ODE can be reduced to a first order linear system.

{\underline{Example}}: $y'' + y = 0$.

We can define a $2 \times 2$ system which is equivalent:
\[ \left\{\begin{array}{l}
     y_1 = y\\
     y_2 = y'
   \end{array}\right. \]
Then:
\[ \left\{\begin{array}{l}
     y_1' = y_2\\
     y_2' = y'' = - y_1
   \end{array}\right. \]
Write in matrix form:
\[ \left[\begin{array}{c}
     y_1\\
     y_2
   \end{array}\right]' = \left[\begin{array}{cc}
     0 & 1\\
     - 1 & 0
   \end{array}\right]  \left[\begin{array}{c}
     y_1\\
     y_2
   \end{array}\right] \]
We can see that $y_1 = \sin x, y_2 = \cos x$ solve{} this system.

{\underline{Another example}}: $y^{(3)} - 2 y'' + 3 y' + 4 y = \sin x$

This is equivalent to the following $3 \times 3$ system:
\[ \left\{\begin{array}{l}
     y_1 = y\\
     y_2 = y'\\
     y_3 = y''
   \end{array}\right. \]
Get:
\[ \left\{\begin{array}{l}
     y_1' = y_2\\
     y_2' = y_3\\
     y_3' = - 4 y_1 + - 3 y_2 + 2 y_3 + \sin x
   \end{array}\right. \]
In matrix form:
\[ \left[\begin{array}{c}
     y_1\\
     y_2\\
     y_3
   \end{array}\right]' = \left[\begin{array}{ccc}
     0 & 1 & 0\\
     0 & 0 & 1\\
     - 4 & - 3 & 2
   \end{array}\right] \left[\begin{array}{c}
     y_1\\
     y_2\\
     y_3
   \end{array}\right] + \left[\begin{array}{c}
     0\\
     0\\
     \sin x
   \end{array}\right] \]
{\underline{Another example}}
\[ y''' + 7 xy'' - \sin xy' + 4 y = 0 \]
We define
\[ \left\{\begin{array}{l}
     y_1 = y\\
     y_2 = y'\\
     y_3 = y''
   \end{array}\right. \]
and obtain the following system:
\[ \left\{\begin{array}{l}
     y_1' = y_2\\
     y_2' = y_3\\
     y_3' = y''' = - 4 y_1 + \sin xy_2 - 7 xy_3
   \end{array}\right. \]
If we solve the system then $y = y_1$ will solve the ODE.

\subsection{System of algebraic equations}

A system of equations of the form:
\[ \left\{\begin{array}{l}
     a_{11} x_1 + \cdots + a_{1 n} x_n = b_1\\
     \vdots\\
     a_{m 1} x_1 + \cdots + a_{m \comma n} x_n = b_n
   \end{array}\right. \]
or in matrix form:
\[ A \left[\begin{array}{c}
     x_1\\
     \vdots\\
     x_n
   \end{array}\right] = \left[\begin{array}{c}
     b_1\\
     \vdots\\
     b_n
   \end{array}\right] \]
or
\[ A \vec{x} = \vec{b} \]
To solve, use Gaussian elimination on the {\tmem{extended matrix}} $(A|
\vec{b})$ to get an equivalent system (with the same set of solutions) in
(upper) echelon form. Then, solve by back substitution.

A matrix is in {\tmem{row echelon form}} if all rows consisting of only zeros
are at the bottom, and a leading non-zero coefficient (``pivot'') of a row is
always strictly to the right of any leading coefficient of the row above. For
example:
\[ \left[\begin{array}{ccccc}
     1 & a_0 & a_1 & a_2 & a_3\\
     0 & 0 & 2 & a_4 & a_5\\
     0 & 0 & 0 & 1 & a_6\\
     0 & 0 & 0 & 0 & 0
   \end{array}\right] \]
Gaussian elimination uses elementary row operations. Three types of
operations:
\begin{enumerate}
  \item Permute rows. ($R_i \leftrightarrow R_j$)
  
  \item Replace row $i$ by a non-zero multiple of itself. ($\alpha R_i
  \rightarrow R_i, \quad \alpha \neq 0$)
  
  \item Add a multiple of row $i$ to row $j$. \ ($\alpha R_i + R_j \rightarrow
  R_j$, any $\alpha$)
\end{enumerate}
{\underline{Note}}: $r$ is the rank of a matrix, and is equal to the number of
non-zero rows in a matrix {\tmem{after}} bringing it to upper echelon form.

{\underline{Definition}}: For an $m \times n$ matrix $A$, we define
fundamental subspaces:
\begin{enumerate}
  \item Set of solutions to $A \vec{x} = \vec{0}$ is a subspace of
  $\mathbb{R}^n$ (= null-space of $A$).
  
  Set of solutions to $A \vec{x} = \vec{b}$ ($\vec{b} \neq \vec{0}$) is not a
  subspace
  
  \item Span of rows of $A$ = row-space of A is a subspace of $\mathbb{R}^n$
  
  \item Span of columns of $A$ = column-space of $A$, is a subspace of
  $\mathbb{R}^m$.
\end{enumerate}
{\underline{Theorems}}:
\begin{itemize}
  \item Rank of $A = r$ is the dimension of row-space and dimension of the
  column-space.
\end{itemize}
\begin{itemize}
  \item Dimension of the null-space is $n - r$.
\end{itemize}
{\underline{Example}}
\[ \left[\begin{array}{cccccc}
     0 & 1 & 2 & - 1 & 0 & 4\\
     0 & 0 & 0 & 3 & 1 & 2\\
     0 & 0 & 0 & 0 & - 1 & 4
   \end{array}\right] \left[\begin{array}{c}
     x_1\\
     \vdots\\
     x_6
   \end{array}\right] = \left[\begin{array}{c}
     0\\
     1\\
     - 3
   \end{array}\right] \]
{\tmstrong{Dependent variables}} are the variables which correspond to the
{\tmstrong{pivot}} columns, in our case $x_2, x_4, x_5$.

Independent variables are all the rest, in our case $x_1, x_3, x_6$.

We want to express the solution in terms of the independent variables.
\begin{eqnarray*}
  - x_5 + 4 x_6 & = & - 3 \rightarrow x_5 = 4 x_6 + 3\\
  3 x_4 + x_5 + 2 x_6 & = & 1 \rightarrow x_4 = \frac{1}{3} (1 - x_5 - 2 x_6)
  = \frac{1}{3} (1 - (4 x_6 + 3) - 2 x_6) = - 2 x_6 - \frac{2}{3}\\
  x_2 + 2 x_3 - x_4 + 4 x_6 & = & 0 \rightarrow x_2 = - 2 x_3 + x_4 - 4 x_6 =
  \cdots = - 2 x_3 - 6 x_6 - \frac{2}{3}
\end{eqnarray*}
General solution:
\[ \left[\begin{array}{c}
     x_1\\
     - 2 x_3 - \frac{2}{3} - 6 x_6\\
     x_3\\
     - 2 x_6 - \frac{2}{3}\\
     4 x_6 + 3\\
     x_6
   \end{array}\right] = x_1 \left[\begin{array}{c}
     1\\
     0\\
     0\\
     0\\
     0\\
     0
   \end{array}\right] + x_3 \left[\begin{array}{c}
     0\\
     - 2\\
     1\\
     0\\
     0\\
     0
   \end{array}\right] + x_6 \left[\begin{array}{c}
     0\\
     - 6\\
     0\\
     - 2\\
     4\\
     1
   \end{array}\right] + \left[\begin{array}{c}
     0\\
     - \frac{2}{3}\\
     0\\
     - \frac{2}{3}\\
     3\\
     0
   \end{array}\right], \quad x_1, x_3, x_6 \in \mathbb{R} \]
Note that the last vector solves the inhomogeneous equation, and the first
three vectors solve the associated homogeneous system. Also, they are linearly
independent and therefore span the basis of the null-space.

{\underline{Another approach}}: First solving the associated homogeneous
system:
\[ \left[\begin{array}{cccccc}
     0 & 1 & 2 & - 1 & 0 & 4\\
     0 & 0 & 0 & 3 & 1 & 2\\
     0 & 0 & 0 & 0 & - 1 & 4
   \end{array}\right] \left[\begin{array}{c}
     x_1\\
     \vdots\\
     x_6
   \end{array}\right] = \left[\begin{array}{c}
     0\\
     0\\
     0
   \end{array}\right] \]
\[ \begin{array}{lll}
     - x_5 + 4 x_6 & = & 0 \rightarrow x_5 = 4 x_6\\
     3 x_4 + x_5 + 2 x_6 & = & 0 \rightarrow x_4 = - 2 x_y\\
     x_2 + 2 x_3 - x_4 + 4 x_6 & = & 0 \rightarrow x_2 = - 2 x_3 - 6 x_3
   \end{array} \]
The set of solutions (null-space of the matrix) is a subspace of
$\mathbb{R}^6$.

The number of free variables is the dimension: \#columns -- rank(coeff.
matrix) = 3

We can construct a basis for this by taking in turn on free variable = 1 and
the rest 0.
\begin{enumerate}
  \item $x_1 = 1, x_3 = x_6 = 0$. $\left[\begin{array}{c}
    1\\
    0\\
    0\\
    0\\
    0\\
    0
  \end{array}\right]$
  
  \item $x_3 = 1, x_1 = x_6 = 0$. $\left[\begin{array}{c}
    0\\
    - 2\\
    1\\
    0\\
    0\\
    0
  \end{array}\right]$
  
  \item $x_6 = 1, x_1 = x_3 = 0$. $\left[\begin{array}{c}
    0\\
    - 6\\
    0\\
    - 2\\
    4\\
    1
  \end{array}\right]$
\end{enumerate}
These vectors are linearly independent and span the space of solutions.

General solution (of the associated homogeneous system) is:
\[ x_1 \cdot \left[\begin{array}{c}
     1\\
     0\\
     0\\
     0\\
     0\\
     0
   \end{array}\right] + x_3 \cdot \left[\begin{array}{c}
     0\\
     - 2\\
     1\\
     0\\
     0\\
     0
   \end{array}\right] + x_6 \cdot \left[\begin{array}{c}
     0\\
     - 6\\
     0\\
     - 2\\
     4\\
     1
   \end{array}\right] \]


To solve the inhomogeneous system, we just need to find a particular solution.
We had the general solution:
\[ \left[\begin{array}{c}
     x_1\\
     - 2 x_3 - \frac{2}{3} - 6 x_6\\
     x_3\\
     - 2 x_6 - \frac{2}{3}\\
     4 x_6 + 3\\
     x_6
   \end{array}\right] = x_1 \left[\begin{array}{c}
     1\\
     0\\
     0\\
     0\\
     0\\
     0
   \end{array}\right] + x_3 \left[\begin{array}{c}
     0\\
     - 2\\
     1\\
     0\\
     0\\
     0
   \end{array}\right] + x_6 \left[\begin{array}{c}
     0\\
     - 6\\
     0\\
     - 2\\
     4\\
     1
   \end{array}\right] + \left[\begin{array}{c}
     0\\
     - \frac{2}{3}\\
     0\\
     - \frac{2}{3}\\
     3\\
     0
   \end{array}\right], \quad x_1, x_3, x_6 \in \mathbb{R} \]
It is clean then that $\left[\begin{array}{c}
  0\\
  - \frac{2}{3}\\
  0\\
  - \frac{2}{3}\\
  3\\
  0
\end{array}\right]$ is a particular solution.

In general, given $A$ $m \times n$ matrix such that $A \vec{x} = \vec{b}$ and
a particular solution $v \in \mathbb{R}^n$, the set of all solutions is the
set $\left\{ v + v_0 | \text{ $v_0 \text{ is in null-space of $A$}$}
\right\}$.

{\underline{Theorem}}: For an $m \times n$ matrix $A$, $n = \dim N (A) + \dim
\left( \text{row space} \right)$ such that $n - r = \dim (\tmop{nullspace})$.
[dim of row space is equal to the dim of column space of $A$.]

{\underline{2 more examples}}:
\begin{enumerate}
  \item
  \[ \left[\begin{array}{ccc|c}
       1 & - 1 & 3 & 1\\
       - 1 & 1 & - 2 & 1\\
       2 & - 1 & 3 & - 4
     \end{array}\right] \rightarrow \cdots \rightarrow
     \left[\begin{array}{ccc|c}
       1 & - 2 & 3 & 1\\
       0 & - 1 & 1 & 2\\
       0 & 0 & 0 & 0
     \end{array}\right] \]
  Can solve. Dimension of null-space is 1 ($n = 3, r = 2$).
  
  \item
  \[ \left[\begin{array}{ccc|c}
       1 & - 2 & 3 & 1\\
       - 1 & 1 & - 2 & 0\\
       2 & - 1 & 3 & 0
     \end{array}\right] \rightarrow \cdots \rightarrow
     \left[\begin{array}{ccc|c}
       1 & - 2 & 3 & 1\\
       0 & - 1 & 1 & 1\\
       0 & 0 & 0 & 1
     \end{array}\right] \]
  $r > n$, No solution!
\end{enumerate}
{\underline{Note}}: There exist solutions to $A \vec{x} = \vec{b}$ if and only
if $\tmop{rank} (A) = \tmop{rank} (A|B)$.

{\newpage}

\subsubsection{Algorithm to invert (square) matrices: Gauss-Seidel algorithm}

Works for {\tmem{square matrices}} ($n \times n$).

{\underline{Definition}}: An elementary matrix is obtained from $I$ (identity
matrix) by performing one elementary operation on its rows. e.g.
\[ \left[\begin{array}{ccc}
     1 & 0 & 0\\
     0 & 1 & 0\\
     - 2 & 0 & 1
   \end{array}\right] \xleftarrow{- 2 R_1 + R_3 \rightarrow R_3} I \]
{\underline{Fact}}: Multiplying a matrix $A$ on the left by an elementary
matrix $E$ performs the same operation defining $E$ on rows of $A$.
\[ \left[\begin{array}{ccc}
     1 & 0 & 0\\
     0 & 1 & 0\\
     - 2 & 0 & 1
   \end{array}\right] \left[\begin{array}{ccc}
     a_{11} & a_{12} & a_{13}\\
     a_{21} & a_{22} & a_{23}\\
     a_{31} & a_{32} & a_{33}
   \end{array}\right] = \left[\begin{array}{ccc}
     a_{11} & a_{12} & a_{13}\\
     a_{21} & a_{22} & a_{23}\\
     - 2 a_{11} + a_{31} & - 2 a_{12} + a_{32} & - 2 a_{13} + a_{33}
   \end{array}\right] \]
{\underline{Idea}}: If we perform elementary operation on rows of $A$ until we
reach $I$, it is as though we multiplied $A$ by a sequence of elementary
matrices.
\[ \underbrace{E_n \cdots E_2 E_1}_{A^{- 1}} A = I \]
{\underline{Algorithm}}

Take $(A|I)$ $n \times 2 n$ matrix and perform elementary operations until $
(I|A^{- 1})$ is obtained.

If the algorithm ``gets stuck'' (don't have enough pivots) then $\tmop{rank}
(A) < n$ and $A$ is not invertible.

{\underline{Note}}: Method works iff $\tmop{rank} A = n \Longleftrightarrow A$
is invertible.

{\underline{Example}}
\[ \left[\begin{array}{ccc|ccc}
     1 & - 1 & 2 & 1 & 0 & 0\\
     1 & 2 & 1 & 0 & 1 & 0\\
     3 & 1 & 4 & 0 & 0 & 1
   \end{array}\right] \xrightarrow{\begin{array}{l}
     - R_1 + R_2 \rightarrow R_2\\
     - 3 R_1 + R_3 \rightarrow R_3
   \end{array}} \left[\begin{array}{ccc|ccc}
     1 & - 1 & 2 & 1 & 0 & 0\\
     0 & 3 & - 1 & - 1 & 1 & 0\\
     0 & 4 & - 2 & - 3 & 0 & 1
   \end{array}\right] \xrightarrow{\frac{1}{3} R_2 \rightarrow R_2} \cdots \]
\[ \left[\begin{array}{ccc|ccc}
     1 & - 1 & 2 & 1 & 0 & 0\\
     0 & 1 & - \frac{1}{3} & - \frac{1}{3} & \frac{1}{3} & 0\\
     0 & 4 & - 2 & - 3 & 0 & 1
   \end{array}\right] \xrightarrow{- 4 R_2 + R_3 \rightarrow R_3}
   \left[\begin{array}{ccc|ccc}
     1 & - 1 & 2 & 1 & 0 & 0\\
     0 & 1 & - \frac{1}{3} & - \frac{1}{3} & \frac{1}{3} & 0\\
     0 & 0 & - \frac{2}{3} & - \frac{5}{3} & - \frac{4}{3} & 1
   \end{array}\right] \]
Notice that now we know that $r = 3$. Eventually, after some operations, we
get:
\[ A^{- 1} = \left[\begin{array}{ccc}
     - \frac{7}{2} & - 3 & - \frac{5}{2}\\
     \frac{1}{2} & 1 & - \frac{1}{2}\\
     \frac{5}{2} & 2 & - \frac{3}{2}
   \end{array}\right] \]

\subsubsection{Review of determinants}

Assume throughout this subsection that all matrices are square.

{\underline{Note}}: If $A$ is an $n \times n$ real matrix, $\det A$ is a real
number.

{\underline{We define $\tmop{def} A = | A |$ recursively}}:

if $n = 1$: $A = \left[\begin{array}{c}
  a
\end{array}\right]$ then $\det A = a$.

For an\quad$n \times n$ matrix $A = \left[\begin{array}{c}
  a_{i \nocomma j}
\end{array}\right]$, denote by $A_{\tmop{ij}}$ = det of matrix obtained from
$A$ by omitting row $i$ and column $j$ ($i \nocomma j^{\tmop{th}}$ minor of
$A$).

Define: $\det A = \sum_{j = 1}^n (- 1)^{i + j} a_{i \nocomma j} A_{i \nocomma
j}$ for any fixed $i$ (``along row $i$'').

{\underline{if $n = 2$}}:
\[ A = \left[\begin{array}{cc}
     a_{11} & a_{12}\\
     a_{21} & a_{22}
   \end{array}\right] \]
Along row 1:
\[ \det A = \sum_{j = 1}^2 a_{1 j} A_{1 j} \cdot (- 1)^{1 + j} = a_{i \nocomma
   j} A_{11} - a_{12} A_{12} = a_{11} a_{22} - a_{12} a_{21} \]
Along row 2:
\[ \det A = \sum_{j = 1}^2 a_{2 j} A_{2 j} \cdot (- 1)^{2 + j} = - a_{21}
   A_{21} + a_{22} A_{22} = - a_{21} a_{12} + a_{22} a_{11} \]
For $2 \times 2$ matrices you can also cross-multiply diagonals and subtract
main from sub-diagonal.

{\underline{if $n = 3$}}:
\[ A = \left[\begin{array}{ccc}
     a_{11} & a_{12} & a_{13}\\
     a_{21} & a_{22} & a_{23}\\
     a_{31} & a_{32} & a_{33}
   \end{array}\right] \]
\begin{eqnarray*}
  \det A & = & \sum_{j = 1}^3 (- 1)^{1 + j} a_{1 j} A_{1 j} = a_{11} A_{11} -
  a_{12} A_{12} + a_{13} A_{13}\\
  & = & a_{11}  \left|\begin{array}{cc}
    a_{22} & a_{23}\\
    a_{32} & a_{33}
  \end{array}\right| - a_{12}  \left|\begin{array}{cc}
    a_{21} & a_{23}\\
    a_{31} & a_{33}
  \end{array}\right| + a_{13}  \left|\begin{array}{cc}
    a_{21} & a_{22}\\
    a_{31} & a_{32}
  \end{array}\right|\\
  & = & a_{11}  (a_{22} a_{33} - a_{23} a_{32}) - a_{12}  (a_{21} a_{33} -
  a_{23} a_{31}) + a_{13}  (a_{21} a_{32} - a_{22} a_{31})\\
  & = & (a_{11} a_{22} a_{33} + a_{12} a_{23} a_{31} + a_{13} a_{21} a_{32})
  - (a_{11} a_{23} a_{32} + a_{12} a_{21} a_{33} + a_{13} a_{22} a_{31})
\end{eqnarray*}
{\underline{Theorem}}: We can express a det also using columns: $\det A =
\sum_{i = 1}^n  (- 1)^{i + j} a_{i \nocomma j} A_{i \nocomma j}$ for a fixed
$j$ (``down column $j$'').

\

\marginpar{Lec 15 25.12.22}{\underline{if $n = 4$}}:

Along row $i$,
\begin{eqnarray*}
  \det A = \sum_{j = 1}^4 (- 1)^{i + j} a_{i \nocomma j} A_{i \nocomma j} & =
  & (- 1)^{i + 1} a_{i \nocomma 1} A_{i \nocomma 1} + (- 1)^{i + 2} a_{i
  \nocomma 2} A_{i \nocomma 2} + (- 1)^{i + 3} a_{i \nocomma 3} A_{i \nocomma
  3} + \cdots + + (- 1)^{i + 4} a_{i \nocomma 4} A_{i \nocomma 4}
\end{eqnarray*}
In fact: $\det A$ = sum of all choices of products of elements chosen, one
from each row and one from each column, multiplied by a power of $(- 1)$. In
total, $n!$ summands.

\subsubsection{Cramer's rule}

{\underline{A strategy to find solutions of a system based on determinant:}}

In a $2 \times 2$ case, where
\[ \left\{\begin{array}{l}
     a_{11} x_1 + a_{12} x_2 = b_1\\
     a_{21} x_1 + a_{22} x_2 = b_2
   \end{array}\right. \]
Operate $R_2 - R_1 \rightarrow R_2$
\[ \left\{\begin{array}{l}
     a_{21} a_{11} x_1 + a_{21} a_{12} x_2 = a_{21} b_1\\
     a_{11} a_{21} x_1 + a_{11} a_{22} x_2 = a_{11} b_2
   \end{array}\right. \]
Subtract
\[ (a_{11} a_{22} - a_{21} a_{12}) x_2 = a_{11} b_2 - a_{21} b_1 \]
If $\det A \neq 0$ we can solve uniquely:
\[ x_2 = \frac{\left|\begin{array}{cc}
     a_{11} & b_1\\
     a_{21} & b_2
   \end{array}\right|}{\det A} \]
It turns out that
\[ x_1 = \frac{\left|\begin{array}{cc}
     b_1 & a_{12}\\
     b_2 & a_{22}
   \end{array}\right|}{\det A} \]
This is called Cramer's rule for $n \times n$ systems. This method is
practical only for small matrices, as it makes us calculate $n + 1$
determinants to find the solutions.

\subsubsection{Properties of determinants}

\begin{enumerate}
  \item If $A$ has a row/column of zeros then $\det A = 0$.
  
  \item $\det I$ = 1. ($I$ is the identity matrix)
  
  \item $\det (c \cdot A) = c^n \det A$, where $c \in \mathbb{C}$.
  
  \item $\det A = \det A^T$.
  
  \item $\det (A \cdot B) = \det A \cdot \det B$.
  
  \item $\det (A + B) \neq \det A + \det B$ (proof by splitting $I$ to
  $\left[\begin{array}{cc}
    1 & 0\\
    0 & 0
  \end{array}\right]$ and $\left[\begin{array}{cc}
    0 & 0\\
    0 & 1
  \end{array}\right]$).
\end{enumerate}
Note that from 2. and 5. $\det A^{- 1} = \frac{1}{\det A}$.

How do elementary row/column operations affect $\det A$?
\begin{enumerate}
  \item If $B$ is obtained from $A$ by one row/column switch, then $\det B = -
  \det A$.
  
  \item If $B$ is obtained from $A$ by multiplying a row/column by a scalar
  $c$, then $\det B = c \cdot \det A$.
  
  \item If $B$ is obtained from $A$ by $\alpha R_i + R_j \rightarrow R_j$,
  then $\det B = \det A$.
  
  \item If $A$ is upper-tridiagonal $\left[\begin{array}{ccc}
    a_{11} & a_{12} & a_{13}\\
    0 & \ddots & \vdots\\
    0 & 0 & a_{n \nocomma n}
  \end{array}\right]$ (elements bellow the main diagonal are only zeros), then
  $\det A = \prod_{i = 1}^n a_{i \nocomma i}$.
\end{enumerate}
{\underline{Conclude:}}

If $\det A \neq 0$ and we use Gaussian elimination on the rows of $A$ to get
it in upper echelon form, $U$, then $\det U \neq 0$. So $U$ will have no rows
of zeros, so {$\tmop{rank} U = \tmop{rank} A = {} n$}.

That means that all rows/columns of $A$ are linearly independent!

Note also that if we had $\det A = 0$, then $\det U = 0$ and $\tmop{rank} U =
\tmop{rank} A < n$.

\begin{tmornamented}
  {\underline{Theorem}}:
  
  Rows/columns of $A$ are linearly independent iff $\det A \neq 0$, iff
  $\tmop{rank} A = n$ iff $A$ is invertible (non-singular).
  
  If $\det A \neq 0$ we can write:
  \[ i \nocomma j^{\tmop{th}} \tmop{element} \tmop{of} A^{- 1}  =
     \frac{1}{\det A}  \underbrace{((- 1)^{i + j} A_{j \nocomma i})
     }_{\text{adjoint of $A$}} \]
  $(- 1)^{i + j} A_{j \nocomma i}$ represents the $i \nocomma j^{\tmop{th}}$
  element of $\tmop{Adj} A$.
\end{tmornamented}

For $n = 2$:
\[ \left[\begin{array}{cc}
     a_{11} & a_{12}\\
     a_{21} & a_{22}
   \end{array}\right]^{- 1} = \frac{1}{a_{11} a_{22} - a_{21} a_{12}} 
   \left[\begin{array}{cc}
     A_{11} & - A_{21}\\
     - A_{12} & A_{22}
   \end{array}\right] = \frac{1}{a_{11} a_{22} - a_{21} a_{12}} 
   \left[\begin{array}{cc}
     a_{22} & - a_{12}\\
     - a_{21} & a_{11}
   \end{array}\right] \]
This is used in Cramer's rule: suppose you have a system of equations $A
\vec{x} = \vec{b}$, then if $\det A \neq 0$ we have a unique solution $\vec{x}
= A^{- 1}  \vec{b}$. Each element of $\vec{x}$ is obtained by:
\[ x_i = \frac{\text{det $A$ where column $i$ is replaced by } \vec{b}}{\det
   A} \]
Different ways of calculating determinant of a $4 \times 4$ matrix $A$:
\[ A = \left[\begin{array}{cccc}
     1 & 0 & - 1 & 2\\
     2 & 3 & 1 & 1\\
     1 & 0 & 2 & 1\\
     3 & 1 & 1 & - 1
   \end{array}\right] \]
Calculate down column 2.
\begin{eqnarray*}
  \det A = \sum_{i = 1}^4 (- 1)^{i + 2} a_{i \nocomma 2} A_{i \nocomma 2} & =
  & (- 1)^{1 + 2} a_{12} A_{12} + (- 1)^{2 + 2} a_{22} A_{22} + (- 1)^{3 + 2}
  a_{32} A_{32} + (- 1)^{4 + 2} a_{42} A_{42}
\end{eqnarray*}
Notice that $a_{12}, a_{42} = 0$
\begin{eqnarray*}
  \det A & = & 3 \cdot \left|\begin{array}{ccc}
    1 & - 1 & 2\\
    1 & 2 & 1\\
    3 & 1 & - 1
  \end{array}\right| + 1 \cdot \left|\begin{array}{ccc}
    1 & - 1 & 2\\
    2 & 1 & 1\\
    1 & 2 & 1
  \end{array}\right|\\
  & = & 3 ((- 2 - 3 + 2) - (12 + 1 + 1)) + 1 \cdot (1 - 1 + 8 - 2 + 2 - 2) =
  - 45
\end{eqnarray*}
Another approach: use elementary operations of type 3 to eliminate below first
pivot (this does not change the determinant)
\begin{eqnarray*}
  &  & 
\end{eqnarray*}
\[ \left[\begin{array}{cccc}
     1 & 0 & - 1 & 2\\
     2 & 3 & 1 & 1\\
     1 & 0 & 2 & 1\\
     3 & 1 & 1 & - 1
   \end{array}\right] \rightarrow \cdots \rightarrow \det
   \left[\begin{array}{cccc}
     1 & 0 & - 1 & 2\\
     0 & 3 & 3 & - 3\\
     0 & 0 & 3 & - 1\\
     0 & 1 & 4 & - 7
   \end{array}\right] = 3 \det \left[\begin{array}{cccc}
     1 & 0 & - 1 & 2\\
     0 & 1 & 1 & - 1\\
     0 & 0 & 3 & - 1\\
     0 & 1 & 4 & - 7
   \end{array}\right] = \]
\[ = 3 \det \left[\begin{array}{cccc}
     1 & 0 & - 1 & 3\\
     0 & 1 & 1 & - 1\\
     0 & 0 & 3 & - 1\\
     0 & 0 & 3 & - 6
   \end{array}\right] = 3 \det \left[\begin{array}{cccc}
     1 & 0 & - 1 & 2\\
     0 & 1 & 1 & - 1\\
     0 & 0 & 3 & - 1\\
     0 & 0 & 0 & - 5
   \end{array}\right] = 3 \cdot 1 \cdot 1 \cdot 3 \cdot (- 5) = - 45 \]
\subsubsection{Review of eigenvalues and eigenvectors}\marginpar{Lec 16
26.12.22}

Assume $A$ is square $n \times n$. The map that sends a vector in
$\mathbb{R}^n$ $\left[\begin{array}{c}
  x_1\\
  \vdots\\
  x_n
\end{array}\right] $ to $A \left[\begin{array}{c}
  x_1\\
  \vdots\\
  x_n
\end{array}\right]$ is a {\tmem{linear operator}}.

{\underline{Special case}}

$A \left[\begin{array}{c}
  x_1\\
  \vdots\\
  x_n
\end{array}\right]$ is a scalar multiple of $\left[\begin{array}{c}
  x_1\\
  \vdots\\
  x_n
\end{array}\right]$.

Clearly $A \cdot \vec{0} = \vec{0}$. We're interested in a non-zero $\vec{x}$
such that $A \vec{x} = \lambda \vec{x}$ for some scalar $\lambda$.

{\underline{Example}}:
\[ \left[\begin{array}{cc}
     3 & 2\\
     1 & 4
   \end{array}\right]  \left[\begin{array}{c}
     1\\
     1
   \end{array}\right] = \left[\begin{array}{c}
     5\\
     5
   \end{array}\right] = 5 \cdot \left[\begin{array}{c}
     1\\
     1
   \end{array}\right] \]
{\underline{Definition}}: If $\vec{x}$ is a non-zero vector such that $A
\vec{x} = \lambda \vec{x}$ for some scalar $\lambda$, we say it is an
{\tmem{eigenvector}} for $A$ and $\lambda$ is its associated
{\tmem{eigenvalue}}.

To find an eigenvector we need to solve:
\[ A \left[\begin{array}{c}
     x_1\\
     \vdots\\
     x_n
   \end{array}\right] = \lambda \left[\begin{array}{c}
     x_1\\
     \vdots\\
     x_n
   \end{array}\right] \]
This is a non-linear system in $n + 1$ unknowns: $x_1, \ldots, x_n, \lambda$.

Suppose $\vec{x}$ solves $A \vec{x} = \lambda \vec{x}$, then: $(A - \lambda
\cdot I)  \vec{x} = \vec{0}$. So $\vec{x}$ is a solution to the homogeneous
linear system. In other words, $\vec{x}$ is a non-trivial element in the
null-space of $A - \lambda I$.

null-space of $A - \lambda I$ is called the {\tmem{eigenspace}}.

Recall that $\dim \tmop{nullspace} = n - r$, so $n - 1 \geq 1$. In other
words, we have non-trivial solutions to $(A - \lambda I)  \vec{x} = \vec{0}$
iff $n > r$ iff $\det (A - \lambda I) = 0$.

Conclude: $\lambda$ is an eigenvalue iff $| A - \lambda I | = 0$.

{\underline{Example}}: Find eigenvectors and eigenvalues of matrix $A$:
\[ A = \left[\begin{array}{cc}
     3 & 2\\
     1 & 4
   \end{array}\right] \]
\begin{enumerate}
  \item Solve $| A - \lambda I | = 0$.
  \[ \left|\begin{array}{cc}
       3 - \lambda & 2\\
       1 & 4 - \lambda
     \end{array}\right| = 0 \]
  \[ (3 - \lambda)  (4 - \lambda) - 2 = 0 \]
  \[ \lambda^2 - 7 \lambda + 10 = 0 \]
  \[ \lambda_{1, 2} = \frac{7 \pm \sqrt{49 - 40}}{2} = 5, 2 \]
  \item Solve $(A - 5 I)  \vec{x} = \vec{0}$ and $(A - 2 I)  \vec{x} =
  \vec{0}$.
  \begin{enumerate}
    \item $\lambda = 5$:
    \[ \left[\begin{array}{cc}
         3 - 5 & 2\\
         1 & 4 - 5
       \end{array}\right]  \left[\begin{array}{c}
         x\\
         y
       \end{array}\right] = \left[\begin{array}{c}
         0\\
         0
       \end{array}\right] \]
    \[ \left[\begin{array}{cc}
         - 2 & 2\\
         1 & - 1
       \end{array}\right]  \left[\begin{array}{c}
         x\\
         y
       \end{array}\right] = \left[\begin{array}{c}
         0\\
         0
       \end{array}\right] \]
    Note this is a matrix of rank 1, as expected. Solve $x - y = 0$ and get
    set of solutions $\left[\begin{array}{c}
      x\\
      - x
    \end{array}\right]$ for any $x \neq 0$ will be an eigenvector. e.g.
    $\left[\begin{array}{c}
      1\\
      1
    \end{array}\right]$.
    
    \item $\lambda = 2$:
    \[ \left[\begin{array}{cc}
         3 - 2 & 2\\
         1 & 4 - 2
       \end{array}\right]  \left[\begin{array}{c}
         x\\
         y
       \end{array}\right] = \left[\begin{array}{c}
         0\\
         0
       \end{array}\right] \]
    \[ \left[\begin{array}{cc}
         1 & 2\\
         1 & 2
       \end{array}\right]  \left[\begin{array}{c}
         x\\
         y
       \end{array}\right] = \left[\begin{array}{c}
         0\\
         0
       \end{array}\right] \]
    choose an eigenvector $\left[\begin{array}{c}
      - 2\\
      1
    \end{array}\right]$.
  \end{enumerate}
\end{enumerate}
{\underline{More examples}}:
\begin{enumerate}
  \item
  \[ A = \left[\begin{array}{cc}
       1 & 2\\
       - 3 & - 6
     \end{array}\right] \]
  Solve $| A - \lambda I | = 0$
  \[ \left|\begin{array}{cc}
       1 - \lambda & 2\\
       - 3 & - 6 - \lambda
     \end{array}\right| = - (1 - \lambda)  (6 + \lambda) + 6 = \lambda^2 + 5
     \lambda = 0 \]
  The eigenvalues are $\lambda = 0, - 5$. We got $\lambda = 0$ because rank $A
  < 2$. $\lambda = 0$ is an eigenvalue iff $\det A = 0$.
  
  {\underline{$\lambda = 0$}}: Find eigenvectors:
  \[ \left[\begin{array}{cc}
       1 & 2\\
       - 3 & - 6
     \end{array}\right]  \left[\begin{array}{c}
       x\\
       y
     \end{array}\right] = \left[\begin{array}{c}
       0\\
       0
     \end{array}\right] \]
  $x + 2 y = 0$. Get $\left[\begin{array}{c}
    - 2\\
    1
  \end{array}\right]$ basis for eigenspace.
  
  {\underline{$\lambda = - 5$}}:
  \[ \left[\begin{array}{cc}
       6 & 2\\
       - 3 & - 1
     \end{array}\right]  \left[\begin{array}{c}
       x\\
       y
     \end{array}\right] = \left[\begin{array}{c}
       0\\
       0
     \end{array}\right] \]
  $3 x + y = 0$. Get $\left[\begin{array}{c}
    1\\
    - 3
  \end{array}\right]$ basis for eigenspace.
  
  \item What if every non-zero vector is an eigenvector?
  \[ A = \left[\begin{array}{cc}
       3 & 0\\
       0 & 3
     \end{array}\right] \]
  \[ \left[\begin{array}{cc}
       3 & 0\\
       0 & 3
     \end{array}\right]  \left[\begin{array}{c}
       x\\
       y
     \end{array}\right] = \left[\begin{array}{c}
       3 x\\
       3 y
     \end{array}\right] \]
  Solve $| A - \lambda I | = 0$
  \[ \left|\begin{array}{cc}
       3 - \lambda & 0\\
       0 & 3 - \lambda
     \end{array}\right| = (3 - \lambda)^2 = 0 \Longleftrightarrow \lambda = 3
  \]
  Note that setting $\lambda = 3$ means when we solve: $\left[\begin{array}{c}
    A - \lambda I
  \end{array}\right]  \left[\begin{array}{c}
    x\\
    y
  \end{array}\right] = \left[\begin{array}{c}
    0\\
    0
  \end{array}\right]$ we get $\left[\begin{array}{cc}
    0 & 0\\
    0 & 0
  \end{array}\right] \left[\begin{array}{c}
    x\\
    y
  \end{array}\right] = \left[\begin{array}{c}
    0\\
    0
  \end{array}\right]$ and every vector is a solution. Here eigenspace has dim
  2.
  
  \marginpar{lec 17 2.1.23}\item
  \[ A = \left[\begin{array}{cc}
       0 & - 1\\
       1 & 0
     \end{array}\right] \]
  $\det (A - \lambda I) = \lambda^2 + 1$. There are no real eigenvalues. Over
  $\mathbb{C}$ there are $\lambda = \pm \mathi$ and we can find the
  eigenvectors over $\mathbb{C}^2$.
  
  {\underline{Set $\lambda = \mathi$}}
  
  Find nullspace of $\left[\begin{array}{cc}
    - \mathi & - 1\\
    1 & - \mathi
  \end{array}\right]  \left[\begin{array}{c}
    x\\
    y
  \end{array}\right] = \left[\begin{array}{c}
    0\\
    0
  \end{array}\right]$. $- \mathi x - y = 0 \rightarrow \left[\begin{array}{c}
    x\\
    y
  \end{array}\right] = \left[\begin{array}{c}
    1\\
    - \mathi
  \end{array}\right]$.
  
  {\underline{Set $\lambda = - \mathi$}}
  
  Find nullspace of $\left[\begin{array}{cc}
    \mathi & - 1\\
    1 & \mathi
  \end{array}\right]  \left[\begin{array}{c}
    x\\
    y
  \end{array}\right] = \left[\begin{array}{c}
    0\\
    0
  \end{array}\right]$. $\mathi x - y = 0 \rightarrow \left[\begin{array}{c}
    x\\
    y
  \end{array}\right] = \left[\begin{array}{c}
    1\\
    \mathi
  \end{array}\right]$.
  
  Notice that the eigenvectors than correspond to conjugate eigenvalues are
  conjugates of each other.
  
  \item
  \[ A = \left[\begin{array}{cc}
       2 & 1\\
       0 & 2
     \end{array}\right] \]
  $\det (A - \lambda I) = (2 - \lambda)^2$. $\lambda = 2$. Find nullspace of
  $A - \lambda I$:
  \[ \left[\begin{array}{cc}
       0 & 1\\
       0 & 0
     \end{array}\right]  \left[\begin{array}{c}
       x\\
       y
     \end{array}\right] = \left[\begin{array}{c}
       0\\
       0
     \end{array}\right] \]
  The vector $\left[\begin{array}{c}
    1\\
    0
  \end{array}\right]$ is a basis for the eigenspace.
  
  {\underline{Theorem}}: If $A$ is $n \times n$ than $\det (A - \lambda I)$ is
  a polynomial of degree $n$.
  
  {\underline{Theorem}}: If $r_0$ is the multiplicity of an eigenvalue
  $\lambda_0$ in the characteristic polynomial of $A$, then the dimension of
  the corresponding eigenspace is less or equal to $r_0$.
  
  Also, dimension of eigenspace is called {\tmem{geometric multiplicity}},
  and multiplicity of eigenvalue in characteristic polynomial\quad is called
  the {\tmem{algebraic multiplicity}}. So $1 \leq \text{geometric multiplicity
  } \leq \text{ algebraic multiplicity}$.
\end{enumerate}
{\underline{Example}}: Suppose $(\lambda - 3)  (\lambda - 2)^5  (\lambda -
1)^2$ is the characteristic polynomial for a matrix.
\begin{itemize}
  \item $\lambda = 3$: eigenspace has dim 1
  
  \item $\lambda = 2$: eigenspace has $1 \leq \dim \leq 5$.
  
  \item $\lambda = 1$: eigenspace has dim 1 or 2.
\end{itemize}

\end{document}
